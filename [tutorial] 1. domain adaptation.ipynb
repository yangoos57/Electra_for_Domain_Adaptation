{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv('data/book_raw_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673269"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizer, ElectraForMaskedLM\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "\n",
    "generator = ElectraForMaskedLM.from_pretrained('monologg/koelectra-base-v3-generator')\n",
    "\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Adaptation을 위한 Electra Model\n",
    "\n",
    "- Electra-pytorch 라이브러리를 koElectra에 맞게 일부 수정했습니다. \n",
    "\n",
    "- Generator 모델은 `ElectraForMaskedLM`로, Discriminator 모델은 `ElectraForPreTraining`로 불러와야 합니다.\n",
    "\n",
    "- 모델 학습 과정을 이해할 수 있도록 generator의 fake sentence 생성 및 Discriminator의 예측을 출력합니다.\n",
    "\n",
    "- Electra-pytorh 원본 github 주소 : https://github.com/lucidrains/electra-pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 :  이 책은 이제부터 파이썬을 시작하고자 하는 프로그래밍 경험자들에게 가장 적합한 파이썬 입문서입니다\n",
      "Masked token 생성 결과 :  ['[CLS]', '이', '책', '##은', '이제', '##부터', '파이', '##썬', '[MASK]', '시작', '##하', '##고', '##자', '하', '[MASK]', '프로그래밍', '경험', '##자', '[MASK]', '##에', '##게', '가장', '적합', '##한', '파이', '##썬', '입문', '##서', '##입니다', '[SEP]']\n",
      "Generator 예측 결과 :  ['/', '이', '책', '##은', '이제', '##부터', '파이', '##썬', '##을', '시작', '##하', '##고', '##자', '하', '##는', '프로그래밍', '경험', '##자', '##들', '##에', '##게', '가장', '적합', '##한', '파이', '##썬', '입문', '##서', '##입니다', '/']\n",
      "sample token : ['##을', '시작', '##자', '##는', '##들']\n",
      "변경된 문장 :  ['[CLS]', '이', '책', '##은', '이제', '##부터', '파이', '##썬', '<< ##을 >>', '시작', '##하', '##고', '##자', '하', '<< ##는 >>', '프로그래밍', '경험', '##자', '<< ##들 >>', '##에', '##게', '가장', '적합', '##한', '파이', '##썬', '입문', '##서', '##입니다', '[SEP]']\n",
      "예측한 문장 :  ['/', '이', '책', '##은', '이제', '##부터', '파이', '##썬', '<< ##을 >>', '시작', '##하', '##고', '##자', '하', '<< ##는 >>', '프로그래밍', '경험', '##자', '<< ##들 >>', '##에', '##게', '가장', '적합', '##한', '파이', '##썬', '입문', '##서', '##입니다', '/']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results(loss=1.8209502696990967, mlm_loss=0.013269344344735146, disc_loss=0.03615361824631691, gen_acc=1.0, disc_acc=1.0, disc_labels=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]]), disc_predictions=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple('Results', [\n",
    "    'loss',\n",
    "    'mlm_loss',\n",
    "    'disc_loss',\n",
    "    'gen_acc',\n",
    "    'disc_acc',\n",
    "    'disc_labels',\n",
    "    'disc_predictions'\n",
    "])\n",
    "\n",
    "# helpers\n",
    "\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature = 1.):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "\n",
    "# main electra class\n",
    "\n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 4,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [2,3],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "                nn.Linear(discr_dim, 1)\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "        \n",
    "        show_mask =tokenizer.convert_ids_to_tokens(masked_input.data[0].tolist())\n",
    "        print('Masked token 생성 결과 : ',show_mask)\n",
    "\n",
    "        # get generator output and get mlm loss(수정)\n",
    "        logits = self.generator(masked_input, **kwargs).logits\n",
    "\n",
    "        print('Generator 예측 결과 : ',tokenizer.convert_ids_to_tokens(logits.unsqueeze(0).max(dim=-1)[1][0][0].tolist()))\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.squeeze(0),\n",
    "            gen_labels.squeeze(0),\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "        print('sample token :', tokenizer.convert_ids_to_tokens(sampled))\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "\n",
    "        # 결과 보여주기\n",
    "        print_input = tokenizer.convert_ids_to_tokens(disc_input[0].tolist())\n",
    "        \n",
    "        # token 단어 표현하기\n",
    "        for i in range(len(disc_input[0])) :\n",
    "            if show_mask[i] == '[MASK]' :\n",
    "                print_input[i] = '<< '+ print_input[i] +' >>'\n",
    "\n",
    "        print('변경된 문장 : ',print_input)\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator(disc_input, **kwargs).logits\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # 결과 보여주기\n",
    "            print_input = tokenizer.convert_ids_to_tokens(gen_predictions[0].tolist())\n",
    "\n",
    "            # token 단어 표현하기\n",
    "            for i in range(len(disc_input[0])) :\n",
    "                if show_mask[i] == '[MASK]' :\n",
    "                    print_input[i] = '<< '+ print_input[i] +' >>'\n",
    "\n",
    "            print('예측한 문장 : ',print_input)        \n",
    "\n",
    "\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean() + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results((self.gen_weight * mlm_loss + self.disc_weight * disc_loss).item(), mlm_loss.item(), disc_loss.item(), gen_acc.item(), disc_acc.item(), disc_labels, disc_predictions)\n",
    "\n",
    "\n",
    "\n",
    "model = Electra(generator=generator,discriminator=discriminator)\n",
    "\n",
    "\n",
    "raw_str = '이 책은 이제부터 파이썬을 시작하고자 하는 프로그래밍 경험자들에게 가장 적합한 파이썬 입문서입니다'\n",
    "\n",
    "# random sample 추출\n",
    "# raw_data = raw_data[raw_data['list'].str.contains('파이썬') == True]\n",
    "# raw_str = raw_data.sample(1).values.item()\n",
    "\n",
    "\n",
    "print('원본 : ', raw_str)\n",
    "input_data = tokenizer(raw_str,return_tensors='pt')\n",
    "\n",
    "model(input_data['input_ids'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
