{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv('data/pre_book_total.csv')\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201678"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizer, ElectraForMaskedLM\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "\n",
    "generator = ElectraForMaskedLM.from_pretrained('monologg/koelectra-base-v3-generator')\n",
    "\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Adaptation을 위한 Electra Model\n",
    "\n",
    "- Electra-pytorch 라이브러리를 koElectra에 맞게 일부 수정했습니다. \n",
    "\n",
    "- Generator 모델은 `ElectraForMaskedLM`로, Discriminator 모델은 `ElectraForPreTraining`로 불러와야 합니다.\n",
    "\n",
    "- 모델 학습 과정을 이해할 수 있도록 generator의 fake sentence 생성 및 Discriminator의 예측을 출력합니다.\n",
    "\n",
    "- Electra-pytorh 원본 github 주소 : https://github.com/lucidrains/electra-pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Electra-pytorch 라이브러리를 KoElectra에 활용할 수 있도록 일부 변형했습니다.\n",
    "\n",
    "### Electra로 Domain Adaptation을 수행하기 위해 개발했습니다.\n",
    "\n",
    "### Generator 모델은 ElectraForMaskedLM로, Discriminator 모델은 ElectraForPreTraining로 불러와야 합니다.\n",
    "\n",
    "### 더 많은 내용을 알고 싶으신 경우 Domain Adaptation Tutorial을 참고해주세요.\n",
    "\n",
    "### Electra-pytorh 원본 github 주소 : https://github.com/lucidrains/electra-pytorch\n",
    "\n",
    "\n",
    "### Electra-pytorch 라이브러리를 KoElectra에 활용할 수 있도록 일부 변형했습니다.\n",
    "\n",
    "### Electra로 Domain Adaptation을 수행하기 위해 개발했습니다.\n",
    "\n",
    "### Generator 모델은 ElectraForMaskedLM로, Discriminator 모델은 ElectraForPreTraining로 불러와야 합니다.\n",
    "\n",
    "### 더 많은 내용을 알고 싶으신 경우 Domain Adaptation Tutorial을 참고해주세요.\n",
    "\n",
    "### Electra-pytorh 원본 github 주소 : https://github.com/lucidrains/electra-pytorch\n",
    "\n",
    "\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple(\n",
    "    \"Results\",\n",
    "    [\n",
    "        \"loss\",\n",
    "        \"mlm_loss\",\n",
    "        \"disc_loss\",\n",
    "        \"gen_acc\",\n",
    "        \"disc_acc\",\n",
    "        \"disc_labels\",\n",
    "        \"disc_predictions\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# helpers\n",
    "\n",
    "\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "\n",
    "def gumbel_sample(t, temperature=1.0):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "\n",
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "\n",
    "\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer=-2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f\"hidden layer ({self.layer}) not found\"\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f\"hidden layer {self.layer} never emitted an output\"\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# main electra class\n",
    "\n",
    "\n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        tokenizer,\n",
    "        *,\n",
    "        num_tokens=None,\n",
    "        discr_dim=-1,\n",
    "        discr_layer=-1,\n",
    "        mask_prob=0.15,\n",
    "        replace_prob=0.85,\n",
    "        random_token_prob=0.0,\n",
    "        mask_token_id=4,\n",
    "        pad_token_id=0,\n",
    "        mask_ignore_token_ids=[2, 3],\n",
    "        disc_weight=50.0,\n",
    "        gen_weight=1.0,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer=discr_layer),\n",
    "                nn.Linear(discr_dim, 1),\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert (\n",
    "                self.num_tokens is not None\n",
    "            ), \"Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling\"\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(\n",
    "                0, self.num_tokens, input.shape, device=input.device\n",
    "            )\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(\n",
    "            masking_mask * replace_prob, self.mask_token_id\n",
    "        )\n",
    "\n",
    "        # get generator output and get mlm loss(수정)\n",
    "        logits = self.generator(masked_input, **kwargs).logits\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.shape[-1]),\n",
    "            gen_labels.reshape(-1),\n",
    "            ignore_index=self.pad_token_id,\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature=self.temperature)\n",
    "        # print(\"sample token :\", self.tokenizer.convert_ids_to_tokens(sampled))\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator(disc_input, **kwargs).logits\n",
    "        disc_logits_reshape = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits_reshape[non_padded_indices], disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            disc_predictions = torch.round(\n",
    "                (torch.sign(disc_logits_reshape) + 1.0) * 0.5\n",
    "            )\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = (\n",
    "                0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean()\n",
    "                + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "            )\n",
    "\n",
    "            #####\n",
    "\n",
    "            # [PAD] 제거\n",
    "            try:\n",
    "                n = masked_input.data[0].tolist().index(3) + 1\n",
    "            except:\n",
    "                n = None\n",
    "\n",
    "            # masked_sen : Masking한 문장 생성\n",
    "            masked_sen = self.tokenizer.convert_ids_to_tokens(\n",
    "                masked_input.data[0].tolist()[:n]\n",
    "            )\n",
    "\n",
    "            # gen_prd : Generator 문장 예측\n",
    "            # [:n] : padding 제거\n",
    "            gen_prd = self.tokenizer.convert_ids_to_tokens(\n",
    "                logits.unsqueeze(0).max(dim=-1)[1][0][0].tolist()[:n]\n",
    "            )\n",
    "\n",
    "            # gen_chg_sen : Generator 신규 문장 생성\n",
    "            gen_chg_sen = self.tokenizer.convert_ids_to_tokens(\n",
    "                disc_input[0].tolist()[:n]\n",
    "            )\n",
    "\n",
    "            for i in range(len(disc_input[0][:n])):\n",
    "                if masked_sen[i] == \"[MASK]\":\n",
    "                    gen_chg_sen[i] = \"<< \" + gen_chg_sen[i] + \" >>\"\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return (masked_sen, gen_prd, gen_chg_sen), Results(\n",
    "            self.gen_weight * mlm_loss + self.disc_weight * disc_loss,\n",
    "            mlm_loss,\n",
    "            disc_loss,\n",
    "            gen_acc,\n",
    "            disc_acc,\n",
    "            disc_labels,\n",
    "            disc_predictions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "model = Electra(generator=generator,discriminator=discriminator,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "1번째 epoch 실행\n",
      "0 번째\n",
      "Discriminator 예측 정확도---*\n",
      "tensor(0.8843)\n",
      "예측 유무 tensor(29.)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from data import book_corpus\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "\n",
    "def train_epoch(model) :\n",
    "    losses = 0\n",
    "    dataset = book_corpus('data/pre_book_total.csv')\n",
    "\n",
    "    batch_size = 32\n",
    "    max_length = 512\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(dataset,batch_size)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i, sen in enumerate(train_dataloader) :\n",
    "        # weight tie\n",
    "        generator.electra.embeddings.token_type_embeddings = discriminator.electra.embeddings.token_type_embeddings\n",
    "        generator.electra.embeddings.position_embeddings = discriminator.electra.embeddings.position_embeddings\n",
    "\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_data = [re.sub('[^.A-Za-z0-9\\u3130-\\u318F\\uAC00-\\uD7A3]|lt|div|gt',' ',s) for s in sen]\n",
    "        input_data = tokenizer(input_data,return_tensors='pt',max_length=max_length,padding=True,truncation=True).to(device)\n",
    "\n",
    "        prd,result = model(input_data['input_ids'])\n",
    "\n",
    "        result.loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # if i // 2 == 0 and i > 0 :\n",
    "        print(f'{i} 번째')\n",
    "        torch.cuda.empty_cache()\n",
    "        masked_sen = prd[0]\n",
    "        gen_prd = prd[1]\n",
    "        gen_chg_sen = prd[2]\n",
    "\n",
    "        # print('*--- 원본 ---*')\n",
    "        # print(sen[0])\n",
    "        # print('')\n",
    "        # print('*--- Input 단어 Masking ---*')\n",
    "        # print(tokenizer.convert_tokens_to_string(masked_sen[1:-1]))\n",
    "        # print('')\n",
    "        # print('Generator 학습---*')\n",
    "        # print(tokenizer.convert_tokens_to_string(gen_prd[1:-1]))\n",
    "        # print('')\n",
    "        # print('Generator 문장 생성---*')\n",
    "        # print(tokenizer.convert_tokens_to_string(gen_chg_sen[1:-1]))\n",
    "        # print('')\n",
    "        print('Discriminator 예측 정확도---*')\n",
    "        print(result.disc_acc)\n",
    "        print('예측 유무', torch.sum(result.disc_predictions) )\n",
    "        # break\n",
    "        losses += result.loss\n",
    "\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "            \n",
    "        \n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    loss = 0\n",
    "    print('-'*30)\n",
    "    print(f'{epoch}번째 epoch 실행')\n",
    "    start_time = timer()\n",
    "    loss = train_epoch(model)\n",
    "    end_time = timer()\n",
    "\n",
    "    print('----*'*20)\n",
    "    print((f\"Epoch: {epoch},loss: {loss},  \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    print('----*'*20)\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024656512771696016"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
