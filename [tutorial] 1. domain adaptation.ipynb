{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pwd\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/electra_for_fine_tuning')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/koelectra-base-v3-generator/resolve/main/vocab.txt from cache at /Users/yangwoolee/.cache/huggingface/transformers/856da019f758549d6b30449d7ae8a955c7274362047cee669ffa1fcb7929f3cc.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
      "loading file https://huggingface.co/monologg/koelectra-base-v3-generator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-v3-generator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-v3-generator/resolve/main/tokenizer_config.json from cache at /Users/yangwoolee/.cache/huggingface/transformers/9a0609f1cc1c5640b3b0c75cbd05668edc0957a39ec8b4f4e913ef24e3f4f989.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-v3-generator/resolve/main/config.json from cache at /Users/yangwoolee/.cache/huggingface/transformers/09502d228ee8b8eaa9631625672abefcb5e21f94d9dcbecdbfb49c37537479e1.1812bd7f349c3790d6fbe056aca322ae9fcb38f825108b0d8728eafb42b570a5\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"monologg/koelectra-base-v3-generator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-v3-generator/resolve/main/config.json from cache at /Users/yangwoolee/.cache/huggingface/transformers/09502d228ee8b8eaa9631625672abefcb5e21f94d9dcbecdbfb49c37537479e1.1812bd7f349c3790d6fbe056aca322ae9fcb38f825108b0d8728eafb42b570a5\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/koelectra-base-v3-generator/resolve/main/pytorch_model.bin from cache at /Users/yangwoolee/.cache/huggingface/transformers/9716b00864ce18dab1fd385f9cee2f75526636c156f7d4d6d18626f47489707f.fcf2865e98d331b301d3f616f038ef6e9cbd538fe25d5fad8351f891de0e6a48\n",
      "All model checkpoint weights were used when initializing ElectraForMaskedLM.\n",
      "\n",
      "All the weights of ElectraForMaskedLM were initialized from the model checkpoint at monologg/koelectra-base-v3-generator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/config.json from cache at /Users/yangwoolee/.cache/huggingface/transformers/353ae9c3d9daa722551a585351b95934cefcf83155f6ff52a1975fa27863dfe0.9c57bd1e7b894b078a3a8ed91a498ca5fb48334c137fe2ec43e8079db1878f8c\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/pytorch_model.bin from cache at /Users/yangwoolee/.cache/huggingface/transformers/efc891feb2c6ce810e4350a7fb9d7f9f48d16b78cc24f95c34bd2d5d1d4bb9c4.20d7b162691634a30f634433042d1bfde6dea4760555a95e166bfa3a3b415682\n",
      "All model checkpoint weights were used when initializing ElectraForPreTraining.\n",
      "\n",
      "All the weights of ElectraForPreTraining were initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForPreTraining for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizer, ElectraForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "\n",
    "generator = ElectraForMaskedLM.from_pretrained('monologg/koelectra-base-v3-generator')\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "\n",
    "\n",
    "# generator = torch.load('model/generator-4.pt')\n",
    "# discriminator = torch.load('model/discriminator-4.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5b33c921347e3fef\n",
      "Found cached dataset csv (/Users/yangwoolee/.cache/huggingface/datasets/csv/default-5b33c921347e3fef/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054c0a538dfe4acabd06af4d7408e72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-293d316a8dadeefa\n",
      "Found cached dataset csv (/Users/yangwoolee/.cache/huggingface/datasets/csv/default-293d316a8dadeefa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde30c1adadd4f618ecafc42c5d84915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset('csv',data_files='data/book_train_128.csv')\n",
    "validation = load_dataset('csv',data_files='data/book_validation_128.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-5b33c921347e3fef/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d2f2d62e57c718aa.arrow\n",
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-293d316a8dadeefa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c7883d31a01565c7.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sen'], max_length=128, padding=True, truncation=True)\n",
    "\n",
    "train_data_set = train['train'].map(tokenize_function,batch_size=True)\n",
    "validation_data_set = validation['train'].map(tokenize_function,batch_size=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Adaptation을 위한 Electra Model\n",
    "\n",
    "- Electra-pytorch 라이브러리를 koElectra에 맞게 일부 수정했습니다. \n",
    "\n",
    "- Generator 모델은 `ElectraForMaskedLM`로, Discriminator 모델은 `ElectraForPreTraining`로 불러와야 합니다.\n",
    "\n",
    "- 모델 학습 과정을 이해할 수 있도록 generator의 fake sentence 생성 및 Discriminator의 예측을 출력합니다.\n",
    "\n",
    "- Electra-pytorh 원본 github 주소 : https://github.com/lucidrains/electra-pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results(loss=(tensor(11.2349, grad_fn=<AddBackward0>),), mlm_loss=tensor(1.6295, grad_fn=<NllLoss2DBackward0>), disc_loss=tensor(0.1921, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), gen_acc=tensor(0.5556), disc_acc=tensor(0.8789), disc_labels=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]]), disc_predictions=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Electra-pytorch 라이브러리를 KoElectra에 활용할 수 있도록 일부 변형했습니다.\n",
    "\n",
    "### Electra로 Domain Adaptation을 수행하기 위해 개발했습니다.\n",
    "\n",
    "### Generator 모델은 ElectraForMaskedLM로, Discriminator 모델은 ElectraForPreTraining로 불러와야 합니다.\n",
    "\n",
    "### 더 많은 내용을 알고 싶으신 경우 Domain Adaptation Tutorial을 참고해주세요.\n",
    "\n",
    "### Electra-pytorh 원본 github 주소 : https://github.com/lucidrains/electra-pytorch\n",
    "\n",
    "\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple(\n",
    "    \"Results\",\n",
    "    [\n",
    "        \"loss\",\n",
    "        \"mlm_loss\",\n",
    "        \"disc_loss\",\n",
    "        \"gen_acc\",\n",
    "        \"disc_acc\",\n",
    "        \"disc_labels\",\n",
    "        \"disc_predictions\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# 모델 내부에서 활용되는 함수 정의\n",
    "\n",
    "\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "\n",
    "def gumbel_sample(t, temperature=1.0):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "\n",
    "# main electra class\n",
    "\n",
    "\n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        tokenizer,\n",
    "        *,\n",
    "        num_tokens=35000,\n",
    "        mask_prob=0.15,\n",
    "        replace_prob=0.85,\n",
    "        mask_token_id=4,\n",
    "        pad_token_id=0,\n",
    "        mask_ignore_token_ids=[2, 3],\n",
    "        disc_weight=50.0,\n",
    "        gen_weight=1.0,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        num_tokens: 모델 vocab_size\n",
    "        mask_prob: 토큰 중 [MASK] 토큰으로 대체되는 비율\n",
    "        replace_prop:  토큰 중 [MASK] 토큰으로 대체되는 비율(?????)\n",
    "        mask_token_i: [MASK] Token id\n",
    "        pad_token_i: [PAD] Token id\n",
    "        mask_ignore_token_id: [CLS],[SEP] Token id\n",
    "        disc_weigh: discriminator loss의 Weight 조정을 위한 값\n",
    "        gen_weigh: generator loss의 Weight 조정을 위한 값\n",
    "        temperature: gumbel_distribution에 활용되는 arg, 값이 높을수록 모집단 분포와 유사한 sampling 수행\n",
    "        \"\"\"\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "\n",
    "        try : \n",
    "            input = input_ids[\"input_ids\"]\n",
    "        except :\n",
    "             input = input_ids\n",
    "\n",
    "        # ------ 1단계 Input Data Masking --------#\n",
    "\n",
    "        \"\"\"\n",
    "        - Generator는 Bert와 구조도 동일하고 학습하는 방법도 동일함. \n",
    "\n",
    "        - Generator 학습을 위해선 [Masked] 토큰이 필요하므로 input data를 Masking하는 과정이 필요함.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(\n",
    "            masking_mask * replace_prob, self.mask_token_id\n",
    "        )\n",
    "\n",
    "        # ------ 2단계 Masking 된 문장을 Generator가 학습하고 가짜 Token을 생성 --------#\n",
    "\n",
    "        \"\"\"\n",
    "        - Generator를 학습하여 MLM_loss 계산(combined_loss 계산에 활용)\n",
    "        - Generator에서 예측한 문장을 Discriminator 학습에 활용\n",
    "        - ex) 원본 문장 : ~~~\n",
    "              마스킹 문장 : \n",
    "              가짜 문장 :\n",
    "        \"\"\"\n",
    "\n",
    "        # get generator output and get mlm loss(수정)\n",
    "        logits = self.generator(masked_input, **kwargs).logits\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2), gen_labels, ignore_index=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature=self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # ------ 3단계 가짜 Token의 진위여부를 Discriminator가 판단하는 단계 --------#\n",
    "\n",
    "        \"\"\"\n",
    "        - 가짜 문장을 학습해 개별 토큰에 대해 진위여부를 판단\n",
    "        - 진짜 token이라 판단하면 0, 가짜 토큰이라 판단하면 1을 부여\n",
    "        - 정답과 비교해 disc_loss를 계산(combined_loss 계산에 활용)\n",
    "        - combined_loss : 학습의 최종 loss임. 모델은 combined_loss의 최솟값을 얻기 위한 방식으로 학습 진행\n",
    "        \"\"\"\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator(disc_input, **kwargs).logits\n",
    "        disc_logits_reshape = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits_reshape[non_padded_indices], disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # combined loss 계산\n",
    "        # disc_weight을 50으로 주는 이유는 discriminator의 task가 복잡하지 않기 떄문임.\n",
    "        # mlm loss의 경우 vocab_size(=35000) 만큼의 loos 계산을 수행하지만\n",
    "        # disc_loss의 경우 src_token_len 만큼의 loss 계산을 수행한만큼\n",
    "        # loss 값에 큰 차이가 발생함. disc_weight은 이를 보완하는 weight임.\n",
    "        combined_loss = (self.gen_weight * mlm_loss + self.disc_weight * disc_loss,)\n",
    "\n",
    "        # ------ 모델 성능 및 학습 과정을 추적하기 위한 지표(Metrics) 설계 --------#\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # gen mask 예측\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # fake token 진위 예측\n",
    "            disc_predictions = torch.round(\n",
    "                (torch.sign(disc_logits_reshape) + 1.0) * 0.5\n",
    "            )\n",
    "            # generator_accuracy\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            # discriminator_accuracy\n",
    "            disc_acc = (\n",
    "                0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean()\n",
    "                + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "            )\n",
    "\n",
    "        #\n",
    "\n",
    "        return Results(\n",
    "            combined_loss,\n",
    "            mlm_loss,\n",
    "            disc_loss,\n",
    "            gen_acc,\n",
    "            disc_acc,\n",
    "            disc_labels,\n",
    "            disc_predictions,\n",
    "        )\n",
    "\n",
    "model = Electra(generator=generator,discriminator=discriminator,tokenizer=tokenizer)\n",
    "input_data = train_data_set[9999]['input_ids']\n",
    "\n",
    "model(torch.tensor([input_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  3342,  6396,  6406,  2227,  4149,  4200,  2129,  4292,  3244,\n",
       "          4219,  6243, 29365,  4181,  4129,  3178,  6522,  4283,  8328,  4192,\n",
       "          2633,  4292,  3041, 24758,     3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# optimizer = torch.optim.AdamW([{'params' : generator.parameters()},{'params' : discriminator.parameters()}], lr=2e-4, betas=(0.9, 0.999), eps=1e-09,weight_decay=0.1)\n",
    "model = Electra(generator=generator,discriminator=discriminator,tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, TrainerCallback,Trainer,DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy='steps'\n",
    ")\n",
    "\n",
    "class myCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A bare :class:`~transformers.TrainerCallback` that just prints the logs.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_step_begin(self, args, state, control, logs=None, **kwargs):\n",
    "\n",
    "        # 함수 이름.. 언제 시작할지\n",
    "        # log는 설정할 때마다\n",
    "        # arg,state,control은 참고할 수 있는 attribute의 경우임.\n",
    "        # 근데 내가 필요한건 input\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            print(\"\")\n",
    "            print(\n",
    "                f\"{int(state.epoch)}번째 epoch 진행 중 ------- {state.global_step}번째 step 결과\"\n",
    "            )\n",
    "\n",
    "\n",
    "class customtrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    ############# 내용 추가\n",
    "    def step_check(self):\n",
    "        # state는 현 상태를 담는 attribute임.\n",
    "        return self.state.global_step\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        prds,outputs = model(inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        # ############# 내용 추가\n",
    "        if self.step_check() % self.args.logging_steps == 0:\n",
    "            print('')\n",
    "            print('Discriminator 예측 정확도---*')\n",
    "            print('disc_acc : ',round(outputs.disc_acc.item(),3) )\n",
    "            print('----*'*20)\n",
    "            print('0이 되는지 check', torch.sum(outputs.disc_predictions).item())\n",
    "            print('----*'*20)\n",
    "            print('loss : ',round(outputs.loss.item(),3))\n",
    "            print('')\n",
    "\n",
    "            #       #####\n",
    "\n",
    "            # # [PAD] 제거\n",
    "            # try:\n",
    "            #     n = masked_input.data[0].tolist().index(3) + 1\n",
    "            # except:\n",
    "            #     n = None\n",
    "\n",
    "            # # masked_sen : Masking한 문장 생성\n",
    "            # masked_sen = self.tokenizer.convert_ids_to_tokens(\n",
    "            #     masked_input.data[0].tolist()[:n]\n",
    "            # )\n",
    "\n",
    "            # # gen_prd : Generator 문장 예측\n",
    "            # # [:n] : padding 제거\n",
    "            # gen_prd = self.tokenizer.convert_ids_to_tokens(\n",
    "            #     logits.unsqueeze(0).max(dim=-1)[1][0][0].tolist()[:n]\n",
    "            # )\n",
    "\n",
    "            # # gen_chg_sen : Generator 신규 문장 생성\n",
    "            # gen_chg_sen = self.tokenizer.convert_ids_to_tokens(\n",
    "            #     disc_input[0].tolist()[:n]\n",
    "            # )\n",
    "\n",
    "            # for i in range(len(disc_input[0][:n])):\n",
    "            #     if masked_sen[i] == \"[MASK]\":\n",
    "            #         gen_chg_sen[i] = \"<< \" + gen_chg_sen[i] + \" >>\"\n",
    "        #     # step_check = 현 step 파악\n",
    "        #     # args.logging_steps = argument에서 지정한 step 불러오가\n",
    "\n",
    "        #     # batch 중 0 번째 위치한 문장 선택\n",
    "        #     num = 1\n",
    "        #     input_id = inputs.input_ids[num].reshape(-1).data.tolist()\n",
    "        #     output_id = outputs.logits[num].argmax(dim=-1).reshape(-1).data.tolist()\n",
    "        #     attention_mask = inputs.attention_mask[num]\n",
    "\n",
    "        #     # mask가 위치한 idx 추출하기\n",
    "        #     mask_idx = (inputs.input_ids[num] == 4).nonzero().data.reshape(-1).tolist()\n",
    "\n",
    "        #     # padding 제거\n",
    "        #     input_id_without_pad = [\n",
    "        #         input_id[i] for i in range(len(input_id)) if attention_mask[i]\n",
    "        #     ]\n",
    "        #     output_id_without_pad = [\n",
    "        #         output_id[i] for i in range(len(output_id)) if attention_mask[i]\n",
    "        #     ]\n",
    "\n",
    "        #     # id to token\n",
    "        #     # [1:-1] [CLS,SEP] 제거\n",
    "        #     inputs_tokens = self.tokenizer.convert_ids_to_tokens(input_id_without_pad)[\n",
    "        #         1:-1\n",
    "        #     ]\n",
    "        #     outputs_tokens = self.tokenizer.convert_ids_to_tokens(\n",
    "        #         output_id_without_pad\n",
    "        #     )[1:-1]\n",
    "\n",
    "        #     # output mask 부분 표시하기\n",
    "        #     for i in mask_idx:\n",
    "        #         # [CLS,SEP 위치 조정]\n",
    "        #         outputs_tokens[i - 1] = \"[\" + outputs_tokens[i - 1] + \"]\"\n",
    "\n",
    "        #     inputs_sen = self.tokenizer.convert_tokens_to_string(inputs_tokens)\n",
    "        #     outputs_sen = self.tokenizer.convert_tokens_to_string(outputs_tokens)\n",
    "\n",
    "        #     print(f\"input 문장 : {''.join(inputs_sen)}\")\n",
    "        #     print(f\"output 문장 : {''.join(outputs_sen)}\")\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = customtrainer(\n",
    "    model=model.to(device),\n",
    "    train_dataset=train_data_set,\n",
    "    eval_dataset=validation_data_set,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[myCallback],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "generator.save_pretrained('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
