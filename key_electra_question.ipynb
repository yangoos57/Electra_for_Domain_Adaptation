{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/yangwoolee/.cache/torch/sentence_transformers/monologg_koelectra-base-v3-discriminator. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/yangwoolee/.cache/torch/sentence_transformers/monologg_koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import utils as keu # key_electra_utils.py 참고\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('monologg/koelectra-base-v3-discriminator')\n",
    "\n",
    "raw_data = pd.read_csv('data/raw_book_info_list.csv',index_col=0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBert를 활용해 Sentence embedding을 만들때 모델의 Max_seq_len에 영향을 받을까?\n",
    "\n",
    "**궁금한 점**\n",
    "\n",
    "* koelectra 경우 Max_seq_len가 512이므로, 토큰이 512개 이상인 문장은 512개 이후에 존재하는 토큰은 버림\n",
    "\n",
    "* 따라서 문장 하나의 토큰이 512개가 넘으면 앞의 512개의 토큰으로만 sentence embedding을 수행할 것으로 생각\n",
    "\n",
    "* 지금 사용하는 input 데이터는 여러 문장을 하나의 문장으로 합친 다음 사용하므로 모든 문장의 토큰이 512개가 넘음\n",
    "\n",
    "* 이런 이유로 512개 이상의 토큰을 가진 문장의 순서를 바꿈에 따라 출력 결과도 바뀔 것으로 예상.\n",
    "\n",
    "**증명방법**\n",
    "\n",
    "* 지금 사용하는 데이터의 [book_title, book_toc, book_intro, publisher] column은 4종류이며, 4종류의 문장을 하나로 통합해서 하나의 input data로 사용\n",
    "\n",
    "* '파이토치 딥러닝 프로젝트 모음집' 도서의 경우 총 793개의 token으로 구분됨. 개별 column의 token 개수는 각 [5, 316, 277,195] 임.\n",
    "\n",
    "* max_length 512이므로 column 순서로 문장을 합치면 각각 [5, 316, 191,0]개의 토큰을 활용할 것이라 생각\n",
    "\n",
    "* 이를 대조하기 위해 원래의 column 순서를 변경해 [5, 316, 277, 195]의 토큰 개수를 [5, 277, 316, 195]로 변경하여 문장 생성, 512개까지 포함하면 [5, 277, 230, 0]개의 토큰으로 학습할 것이라 생각\n",
    "\n",
    "* 기존 문장을 original_sentence, 새로운 문장은 new_sentence로 지칭하였음\n",
    "\n",
    "* origin_sentence 출력과 new_sentence 출력이 다를 경우 SBert의 sentence_embedding이 모델 max_seq_len에 영향을 받는 다 생각할 수 있음.\n",
    "\n",
    "* origin_sentence 출력과 new_sentence 출력이 같을 경우 어떻게 sentence embedding을 만드는지 추가적인 학습이 필요\n",
    "\n",
    "**결론**\n",
    "* origin_sentence 출력과 new_sentence 출력이 같음\n",
    "* Sentence embedding은 모델 max_seq_len에 영향받지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Sentence 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환한 도서정보 :  파이토치 딥러닝 프로젝트 모음집\n"
     ]
    }
   ],
   "source": [
    "original_data = raw_data.iloc[1100] # 1100 파이토치 딥러닝 프로젝트 모음집\n",
    "\n",
    "book_info :str =keu.Merge_Series_to_str(original_data)\n",
    "book_info_trans = keu.trans_engwords_to_hanwords(book_info)\n",
    "\n",
    "original_sentence = keu.find_han_words(' '.join(book_info_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New sentence 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환한 도서정보 :  파이토치 딥러닝 프로젝트 모음집\n"
     ]
    }
   ],
   "source": [
    "new_data = raw_data.iloc[1100].values.tolist()\n",
    "new_data.append(new_data.pop(1))\n",
    "\n",
    "book_info :str =keu.Merge_Series_to_str(new_data)\n",
    "book_info_trans = keu.trans_engwords_to_hanwords(book_info)\n",
    "\n",
    "new_sentence = keu.find_han_words(' '.join(book_info_trans))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original sentence vs New sentence 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----original----\n",
      "['인공지능', '사례', '머신러닝', '머신', '러닝', '머신러닝이란', '머신러닝', '구분', '지도학습', '러닝', '비지도학습', '러닝', '과적합과', '모델', '학습법', '성능', '지표', '딥러닝', '딥', '러닝', '딥러닝이란', '딥러닝', '발전', '과정', '퍼셉트론', '퍼셉트론', '다층', '퍼셉트론', '레이어', '퍼셉트론', '인공신경망', '핵심', '알고리즘', '고급', '딥러닝', '기술', '뉴럴', '네트워크', '뉴럴', '네트워크']\n",
      "----new----\n",
      "['실제로', '우리가', '흔히', '다루는', '날것의', '데이터를', '활용한', '프로젝트', '예제집은', '찾기가', '어렵습니다', '이', '책은', '딥러닝', '프로젝트들을', '중점적으로', '다룸으로써', '어느정도', '딥러닝', '지식은', '있으나', '프로젝트로', '경험하고', '싶은', '독자들에게', '꼭', '필요한', '책이', '될', '것입니다', '이', '책은', '크게', '이론', '파트와', '실전', '파트로', '나누어져', '있습니다', '이론']\n"
     ]
    }
   ],
   "source": [
    "print('----original----')\n",
    "print(original_sentence[10:50])\n",
    "\n",
    "print('----new----')\n",
    "print(new_sentence[10:50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 비교\n",
    "문장 순서를 바꾸더라도 결과는 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----original----\n",
      "['국민청원', '다양한', '데이터', '데이터를', '딥러닝', '딥러닝을', '러닝', '분류하기', '불러오기', '이론', '있습니다', '전처리', '책으로', '책은', '크롤링', '파이토치', '파헤치기', '퍼셉트론', '프로젝트', '프로젝트로']\n",
      "----new----\n",
      "['국민청원', '다양한', '데이터', '데이터를', '딥러닝', '딥러닝을', '러닝', '분류하기', '불러오기', '이론', '있습니다', '전처리', '책으로', '책은', '크롤링', '파이토치', '파헤치기', '퍼셉트론', '프로젝트로', '프로젝트를']\n"
     ]
    }
   ],
   "source": [
    "origin = keu.key_extraction(original_sentence,model)\n",
    "new = keu.key_extraction(new_sentence,model)\n",
    "print('----original----')\n",
    "print(sorted(origin.index.tolist()))\n",
    "print('----new----')\n",
    "print(sorted(new.index.tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Pre-trained 된 generator를 가지고 어떻게 fake sentence를 만들 수 있을까?\n",
    "\n",
    "**궁금한점**\n",
    "* koElectra로 Domain Adaptation을 위한 설계중 어떻게 generator로 fake sentence를 만들어야 하는징 대한 문제에 직면했음\n",
    "\n",
    "* 무엇보다 Generator는 Bert와 동일한 구조라 생각해 output이 (1,src_token_len, vocab_size)이 나와야 된다고 생각했는데, output shape이 (1,src_token_len, 256)으로 나오는게 의아했음. \n",
    "\n",
    "* huggingface 페이지에 작성한 활용 예시대로 활용할 경우 문제없이 작동하길래 어디서부터 잘못된 것인지 파악이 필요했음.\n",
    "\n",
    "**증명방법**\n",
    "\n",
    "* (1,src_token_len, 256)를 (1,src_token_len, vocab_size)로 변환하기 위해 새로운 모델을 제작\n",
    "\n",
    "* transformers pipeline module을 뜯어보고 어떻게 output을 생성하는지 추적\n",
    "\n",
    "**배운점**\n",
    "\n",
    "* Encoder output을 vocab_size 크기로 변경하는 마지막 layer 또한 학습이 필요함을 경험으로 확인하였음.\n",
    "    * last_hidden_state를 vocab_size 차원으로 확대 한 뒤 사용해보니 그 결과가 엉망이었음.\n",
    "\n",
    "* Transformers 라이브러리에서 불러오는 클래스에 따라 모델의 Output이 달라짐을 이해했음.\n",
    "\n",
    "    * Electramodel을 사용할 경우 Encoder 끝단의 output을 출력함. 출력명은 `last_hidden_state`, 크기는 `(1,src_token_len, 256)`임.\n",
    "    * ElectraForMaskedLM을 사용할 경우 vocab 단어별 확률을 출력함. 출력명은 `logits` 크기는 `(1,src_token_len, vocab_size)` 임.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenizing': ['조건', '##검', '##색', '실시간', '##으로', '[MASK]', '하', '##기']}\n",
      "{'model_input': tensor([[    2,  7080,  4166,  4703, 10460, 10749,     4,  3755,  4031,     3]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, ElectraTokenizer,ElectraModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "\n",
    "raw_str = '조건검색 실시간으로 [MASK]하기'\n",
    "input_data = tokenizer(raw_str,return_tensors='pt')\n",
    "\n",
    "pprint({'tokenizing' : tokenizer.tokenize(raw_str)})\n",
    "pprint({'model_input' : input_data['input_ids']})\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElectraModel을 활용하여 [Mask]를 예측할 경우\n",
    "\n",
    "### 부자연스러운 결과 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_output\n",
      "last_hidden_state :  torch.Size([1, 10, 256])\n",
      "\n",
      "Input_data  조건검색 실시간으로 [MASK]하기\n",
      "--------------------\n",
      "'>>> 조건검색 실시간으로 ##ition하기'\n",
      "'>>> 조건검색 실시간으로 가느다란하기'\n",
      "'>>> 조건검색 실시간으로 〈하기'\n",
      "'>>> 조건검색 실시간으로 ##잡이하기'\n",
      "'>>> 조건검색 실시간으로 연예하기'\n"
     ]
    }
   ],
   "source": [
    "generator = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "\n",
    "token_output = generator(**input_data)\n",
    "\n",
    "print('Model_output')\n",
    "print('last_hidden_state : ',token_output.last_hidden_state.shape) \n",
    "\n",
    "\n",
    "class output_generator(nn.Module) :\n",
    "    '''\n",
    "    last_hidden_state를 vocab_size에 맞게 차원 확장하는 모델 구현\n",
    "    input_size = (1,src_token_size, 256) \n",
    "    output_size = (1,src_token_size, 35000)\n",
    "    '''\n",
    "    def __init__(self, output_embed : int, vocab_size : int) -> None:\n",
    "        super().__init__()\n",
    "        self.output_embed = output_embed\n",
    "        self.vocab_embedding = nn.Linear(output_embed,vocab_size) #(256 => 35000)\n",
    "\n",
    "    def forward(self,src) :\n",
    "        return self.vocab_embedding(src) #(src_token_len, 35000)\n",
    "\n",
    "\n",
    "token_logits = output_generator(256,35000) # (embed_size, vocab_size)\n",
    "token_logits = token_logits(token_output.last_hidden_state)\n",
    "\n",
    "# [mask]가 위치한 idx\n",
    "mask_token_index = torch.where(input_data[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# [mask]가 위치한 vocab\n",
    "mask_token_logits = token_logits[0,mask_token_index-1, :]\n",
    "\n",
    "# top 5추출\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "# 결과 추출\n",
    "print('')\n",
    "print(\"Input_data \", raw_str)\n",
    "print('-'*20)\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {raw_str.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging face generator 페이지에 나온 예제 활용 하는 경우(pipeline 활용)\n",
    "\n",
    "### 정상 작동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.09738744050264359,\n",
      "  'sequence': '조건검색 실시간으로 확인 하기',\n",
      "  'token': 6465,\n",
      "  'token_str': '확인'},\n",
      " {'score': 0.07290185242891312,\n",
      "  'sequence': '조건검색 실시간으로 답변 하기',\n",
      "  'token': 8610,\n",
      "  'token_str': '답변'},\n",
      " {'score': 0.03160829842090607,\n",
      "  'sequence': '조건검색 실시간으로 진행 하기',\n",
      "  'token': 6355,\n",
      "  'token_str': '진행'},\n",
      " {'score': 0.0188735481351614,\n",
      "  'sequence': '조건검색 실시간으로 로그인 하기',\n",
      "  'token': 22864,\n",
      "  'token_str': '로그인'},\n",
      " {'score': 0.017551911994814873,\n",
      "  'sequence': '조건검색 실시간으로 답 하기',\n",
      "  'token': 2358,\n",
      "  'token_str': '답'}]\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"monologg/koelectra-base-v3-generator\",\n",
    "    tokenizer=\"monologg/koelectra-base-v3-generator\"\n",
    ")\n",
    "\n",
    "pprint(fill_mask(raw_str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers class를 ElectraModel에서 ElectraForMaskedLM로 바꿔 사용한 경우\n",
    "\n",
    "### 예제와 같이 정상 작동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForMaskedLM\n",
    "\n",
    "forMaskedLM = ElectraForMaskedLM.from_pretrained('monologg/koelectra-base-v3-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_output\n",
      "logits :  torch.Size([1, 10, 35000])\n",
      "\n",
      "Input_data  조건검색 실시간으로 [MASK]하기\n",
      "--------------------\n",
      "'>>> 조건검색 실시간으로 확인하기'\n",
      "'>>> 조건검색 실시간으로 답변하기'\n",
      "'>>> 조건검색 실시간으로 진행하기'\n",
      "'>>> 조건검색 실시간으로 로그인하기'\n",
      "'>>> 조건검색 실시간으로 답하기'\n"
     ]
    }
   ],
   "source": [
    "raw_str = '조건검색 실시간으로 [MASK]하기'\n",
    "input_data = tokenizer(raw_str,return_tensors='pt')\n",
    "\n",
    "\n",
    "logits = forMaskedLM(**input_data).logits\n",
    "\n",
    "print('Model_output')\n",
    "print('logits : ',logits.shape) \n",
    "\n",
    "\n",
    "mask_token_index = torch.where(input_data[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "\n",
    "\n",
    "mask_token_logits = logits[0,mask_token_index, :]\n",
    "\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "print('')\n",
    "print(\"Input_data \", raw_str)\n",
    "print('-'*20)\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {raw_str.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda Out of Memory 문제 해결 : 데이터 전처리를 제대로 해야하는 이유"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GPU 메모리가 급증하는 시기 발견하여 원인을 분석함.\n",
    "\n",
    "* input_data의 shape를 추적한 결과 max_length(=512)에 근접할 경우 OOM이 발생함을 확인함.\n",
    "    ```python\n",
    "    200번째 : torch.Size([16, 450])\n",
    "    ---------------------------------------------------------------------------\n",
    "    OutOfMemoryError                          Traceback (most recent call last)\n",
    "    ```\n",
    "\n",
    "* 지금껏 batch_size에 OOM 문제 발생 시 batch_size를 줄이는데만 신경썼음.\n",
    "\n",
    "* tokenizing 단계에서 max_length를 512->256으로 줄이면 쉽게 문제 해결이 가능했으나,\n",
    "\n",
    "* max_length를 최대한 줄여 batch_size를 향상시킬 수 있는 방법을 고민함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>실무에서 자주 쓰는 파이썬 라이브러리는 다 있다 필수 파이썬 라이브러리 개 엄선 이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  실무에서 자주 쓰는 파이썬 라이브러리는 다 있다 필수 파이썬 라이브러리 개 엄선 이..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import ElectraTokenizer\n",
    "\n",
    "# 전처리 된(줄 알았던) 데이터 로드\n",
    "raw_data = pd.read_csv('data/old_data.csv',index_col=0)\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "\n",
    "raw_data.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 Token 개수 확인\n",
    "* 최대 2081개의 토큰이 생성 가능함을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165865</th>\n",
       "      <td>4456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108710</th>\n",
       "      <td>4353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170827</th>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116711</th>\n",
       "      <td>2478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116735</th>\n",
       "      <td>2081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138205</th>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136695</th>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163103</th>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155702</th>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138977</th>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "165865  4456\n",
       "108710  4353\n",
       "170827  2487\n",
       "116711  2478\n",
       "116735  2081\n",
       "...      ...\n",
       "138205   555\n",
       "136695   550\n",
       "163103   549\n",
       "155702   533\n",
       "138977   528\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(raw_data['0'].tolist())\n",
    "tokens_num = [len(i) for i in tokens['input_ids']]\n",
    "df_token_num = pd.DataFrame(tokens_num)\n",
    "\n",
    "df_token_num = df_token_num.sort_values(by=0,ascending=False)\n",
    "\n",
    "df_token_num.head(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰이 가장 많이 생긴 문장을 찾아본 결과 전처리 수행이 미흡함을 발견\n",
    "\n",
    "**4456개 토큰으로 나눠지는 문장 원본**\n",
    "\n",
    "> \"id\"\" : \"\"SE-ebbe7d35-8e9b-4578-992c-989ae9a3a691\"\", \"\"src\"\" : \"\"http://mblogthumb4.phinf.naver.net/MjAyMDA5MjZfMjkg/MDAxNjAxMDgxNDc0NDc3.nPQjxQCHTJAkIPQrIGovUhhWVsuwnSUCGseyJu4exzEg.pJRoLx5cFKZ8t-lyDDg9q86j_bC9FTfagyWz8sPI-ecg.JPEG.0517park/SE-ebbe7d35-8e9b-4578-992c-989ae9a3a691.jpg\"\", \"\"linkUse\"\" : \"\"false\"\", \"\"link\"\" : \"\"\"\"}\"\" style=\"\"color: #000000; text-decoration: none; cursor: pointer; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border: 0px; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline;\"\" target=\"\"_blank\"\"&gt; id\"\" : \"\"SE-24ea1b1a-cb11-4f7e-8359-8786b04b1a32\"\", \"\"src\"\" : \"\"http://mblogthumb2.phinf.naver.net/MjAyMDA5MjZfMTAx/MDAxNjAxMDgxNDc1MjUx.JWFkCOG6se16qMJ9Cf6pbQ-If0wAk7LBtAnuiDVh3b4g.0RKV2pdWZCSg2weiS96XatrKZriqHfNPztptag4na8og.JPEG.0517park/SE-24ea1b1a-cb11-4f7e-8359-8786b04b1a32.jpg\"\", \"\"linkUse\"\" : \"\"false\"\", \"\"link\"\" : \"\"\"\"}\"\" style=\"\"color: #000000; text-decoration: none; cursor: pointer; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border: 0px; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline;\"\" target=\"\"_blank\"\"&gt; id\"\" : \"\"SE-e66e541b-4f5d-4bba-9232-6dc1bebd0c07\"\", \"\"src\"\" : \"\"http://mblogthumb2.phinf.naver.net/MjAyMDA5MjZfMjI3/MDAxNjAxMDgxNDc1Nzk0.XaPVoOK9D5XdvkFtH5VkaUC8Cw6PpI2qDWo-OQeBb5Mg.eJ8l6E-B4RU_alQ_-uw5fkQbk5km7ZXyTSvsITnYazQg.JPEG.0517park/SE-e66e541b-4f5d-4bba-9232-6dc1bebd0c07.jpg\"\", \"\"linkUse\"\" : \"\"false\"\", \"\"link\"\" : \"\"\"\"}\"\" style=\"\"color: #000000; text-decoration: none; cursor: pointer; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border: 0px; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline;\"\" target=\"\"_blank\"\"&gt; id\"\" : \"\"SE-7f93752e-f031-46d7-9c02-978d64ff9eba\"\", \"\"src\"\" : \"\"http://mblogthumb2.phinf.naver.net/MjAyMDA5MjZfMjM2/MDAxNjAxMDgxNDc2NDkw.LOVPbB03U1tCG2ruFWuHrq_nEVMB3coj5LMcQ7uTMz0g.GH4Mn-N15VzputfCixZy9eE1NQ76GoZU6Y_BVhj5viEg.JPEG.0517park/SE-7f93752e-f031-46d7-9c02-978d64ff9eba.jpg\"\", \"\"linkUse\"\" : \"\"false\"\", \"\"link\"\" : \"\"\"\"}\"\" style=\"\"color: #000000; text-decoration: none; cursor: pointer; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border: 0px; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline;\"\" target=\"\"_blank\"\"&gt; id\"\" : \"\"SE-d2a77837-52d9-4e92-b1b0-c336cf94747f\"\", \"\"src\"\" : \"\"http://mblogthumb1.phinf.naver.net/MjAyMDA5MjZfMjQ3/MDAxNjAxMDgxNDc3MDc5.nd5_sbsTXeuQY-V_tR6b03Ufdy3Bb4ccgOEzmuT8pd8g.Gn28Ki7kKe2gwvCSTvmj3qWX7MEBRuPAw8R6nwEH6Pcg.JPEG.0517park/SE-d2a77837-52d9-4e92-b1b0-c336cf94747f.jpg\"\", \"\"linkUse\"\" : \"\"false\"\", \"\"link\"\" : \"\"\"\"}\"\" style=\"\"color: #000000; text-decoration: none; cursor: pointer; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border: 0px; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline;\"\" target=\"\"_blank\"\"&gt; 다양한 브러시로 그림그리기 연습!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 재진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [['점프 투 파이썬 책의 커리큘럼처럼 이어져서 좋다.', '다만, 사실상 거의 입...\n",
       "2    [['쉽게 설명되어 있어 입문용으로 좋습니다.'], ['도커를 배울 수 있는 최신 ...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리 수행하지 않은 raw_file 불러오기\n",
    "\n",
    "book_reply = pd.read_csv('data/raw_book_reply.csv',index_col= 0)\n",
    "\n",
    "book_reply['2'].head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 리스트 내 리스트 제거를 위한 1단계 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'점프 투 파이썬 책의 커리큘럼처럼 이어져서 좋다.'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast \n",
    "\n",
    "sentences = book_reply[book_reply['2'].isna() == False]['2'].tolist()\n",
    "\n",
    "# str type의 list를 다시 list type으로 전환 \n",
    "# ** list를 csv로 저정할 경우 str type으로 저장되기 때문에 아래의 매서드 수행\n",
    "sentences = list(map(lambda x : ast.literal_eval(x), sentences))\n",
    "\n",
    "# 리스트 내 리스트 제거하기\n",
    "book_reply = [j for i in sentences for j in i]\n",
    "book_reply = [j for i in book_reply for j in i]\n",
    "\n",
    "book_reply[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 최대길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9399</th>\n",
       "      <td>8167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66554</th>\n",
       "      <td>6854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71516</th>\n",
       "      <td>4896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17400</th>\n",
       "      <td>3926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71522</th>\n",
       "      <td>3697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93815</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29683</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29700</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98607</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94402</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102367 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "9399   8167\n",
       "66554  6854\n",
       "71516  4896\n",
       "17400  3926\n",
       "71522  3697\n",
       "...     ...\n",
       "93815     1\n",
       "29683     1\n",
       "29700     1\n",
       "98607     1\n",
       "94402     1\n",
       "\n",
       "[102367 rows x 1 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_len = pd.DataFrame([len(j) for j in book_reply]).sort_values(by=0,ascending=False)\n",
    "check_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 길이를 가진 문장 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-90d7bec4-b38e-4aee-a156-9b16e71b1447\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 사실 저같은 경우에는 수학적인 부분이 상당히 약하고 자신없어서 피할 수 있으면 최대한 피하고 살고 싶었는데요. 아이를 키우고 공부를 시키면서 수학이 우리 생활과 얼마나 밀접하게 연관이 되어 있는지를 알게 되었고요. 아이 문제풀이를 위해 엄마도 수학문제를 풀기 시작했고 무조건 거부하고 피하면 안되겠다는 생각이 들더라고요. 엑셀역시 마찬가지로 기본적인 것들은 할 수 있지만 아주 소소한 부분에 그치고 있고.. 진짜 제가 하고 싶고 배우고 싶은 것은 엑셀함수를 자유자재로 활용하여 유용하게 시간활용을 하고 효율적으로 일처리를 하는 것이서어ㅛ. 한정희 쌤의 책을 만나보게 되었는데 잘 선택했다는 생각이 들더라고요. 저도 구독완료! &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-9f1a36a6-2f65-4a38-a0f1-1cf80922877a\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 특히 이 책의 저자님은 유튜브에 \\'짤막한 강좌\\'를 업로드하신 터라 무료 동영상 160강도 함께 만나볼 수 있어 좋은데요. 책을 보니 책만으로도 충분히 훌륭하고 좋다는 생각이 들어서 엑셀을 공부할 때에 다른 곳에 한눈팔지 말고 이 책과 저자님의 동영상강의를 보면 되겠다 싶더라고요. &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-d95b6940-d89c-4fea-92df-d8b9fa69f9f2\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 책의 앞쪽에는 \"서두르지 마라, 그러나 멈추지도 마라.\"라는 요한 볼프강 폰 괴테의 명언이 적혀 있어서요. 다둥맘인지라 서두를 수 없는 저에게 뭔가 의미심장하게 다가왔는데.. 한 권 한 권 이지스퍼블리싱에서 만나보는 된다! 시리즈들로 제 인생이 조금씩 긍정적인 방향으로 바뀌고 있지 않나 싶습니다. 이 책은 총 셋째 마당으로 이뤄져 있으며.. 첫째 마당: 엑셀의 기본, 데이터 입력과 관리 둘째 마당: 엑셀의 꽃, 수식과 함수 셋째 마당: 보고서에 필수! 데이터 집계와 시각화 &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-442be7c3-ffc7-45a9-968e-8d353fe26780\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 의 순서로 엑셀정복을 하게 되는데요. 이 책에서는 바쁜 직장인을 위한 7일 속성 입문 코스는 물론.. 16일 코스 스케줄도 수록하고 있어서 원하시는대로 진도를 나가보시면 좋을 듯 ! 7일 속성 입문 코스로는 1일 차 : 업무에 많이 쓰인느 양식 만들며 기본 익히기 2일 차: 데이터베이스 관리 및 유효성 검사 3일 차:엑셀의 꽃, 수식과 함수의 기본 4일 차: IF함수와 VLOOKUP함수 5일 차 :데이터 요약과 필터링 6일 차 : 차트 작성의 비밀 7일차 : 엑셀 끝판왕! 피벗 의 순으로 진도를 나가시면 되는데요. 이렇게 7일 속성 입문 코스로 공부하신다면 하면 된다! 위주로 실습을 진행하면 되시고요. 16일이면 책을 꼼꼼히 살펴볼 수 있어서 프로 엑셀러가 될 수 있다니.. 저는 16일 코스로 공부하려고용. &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-cdc4df3c-72e2-4761-b5a7-2b194d988a80\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 잠시 엑셀에 대해서 소개를 해드리면.. 엑셀은 마이크로소프트사에서 만들었으며 수힉과 함수를 사용하여 계산이 필요한 문서를 작성하는 프로그램인데요. 데이터베이스와 같은 많은 데이터를 요약하고 정리할 때에 정말 유용한데.. 비교하거나 강조할 데이터의 다지인 역시 자유자재로 바꿀 수 있어서요. 이를 바탕으로 하여 도표와 분석 차트도 쉽게 작성할 수 있다는 특징이 있답니다. 그리하여 직장에서는 기본적으로 엑셀을 참 많이 활용하게 되는 것 같은데요. 저 역시 잠시 머물던 회사에서 엑셀작업을 많이 해야했던 기억이 있는데.. 저보다 먼저 들어온 선배분이 슬며시 저에게 엑셀 사용방법을 물어보았던 것도 떠오릅니다. &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-d6334f82-19fd-4c0a-9252-f1d76d2f040e\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 사실 아랫사람에게 엑셀을 물어보는 것도 민망하고, 누가 물어봤을 때 잘 알려주거나 대답하지 못하는 것도 민망하다죠. 직장생활을 하게 된다면 많이 사용하는 엑셀의 대표적인 내용들은 미리 알아두고 단축키도 암기해두면 유용할 듯 한데요. 이 책에서는 그러한 부분은 물론 그 이상을 담고 있다는 생각이 들더라고용. &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-779146c1-bd69-4486-8f8f-ff150c1c5d6d\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 요렇게 엑셀을 이용하여 견적서 양식도 만들 수 있고요. 각종 도표와 그래프 등도 만들 수 있으니 회의할 때도 넘 유용하겠더라고용. &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-b0eb3a36-3ec1-4718-8dbb-930bdbbc64e4\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 사실 제가 이전에 했었던 엑셀은 엑셀이 아니라고 하고 싶은 것이.. 편리하게 사용한 것이 아니라 무식하게 입력하며 누구에게 말도 못하고 괴로워했던 것 같은데요. 수많은 데이터베이스를 엑셀의 장점을 활용하여 정리한다면 좋았을텐데 그때에 좀 더 준비를 했어야했다는 생각도 든다는.. &lt;div class=\"se-component se-sticker se-l-default\" id=\"SE-f99effec-95fd-4aff-b120-47ac943cd61d\" style=\"margin: 20px 0px 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; position: relative;\"&gt; &lt;div class=\"se-component-content\" style=\"margin: 0px auto; padding: 0px 40px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; max-width: 100%;\"&gt; &lt;/div&gt; &lt;/div&gt; 여전히 이런 부분을 잘하지 못하지만 첫술에 배부를 수가 있나요. 제 스스로 엑셀책을 찾아본다는 것이 하나의 큰 시도인 것 같은데요. 지금 당장 제가 피벗테이블을 이용하여 그럴싸한 보고용 차트를 만들 필요는 없지만 짬짬이 공부해볼 생각! 아이들이 셋이나 있다보니 이런저런 걱정과 생각이 참 많은 저인데요. 걱정만 하지말고 회사에서 많이 쓰이는 실무엑셀을 잘 공부해둬서 나중에 어떠한 기회가 찾아온다면 잘 잡고 싶다는요. 그럼 저는 여기까지! 이지스퍼블리싱 &lt;된다! 7일 실무 엑셀&gt; 이었습니다. &lt;p class=\"se-text-paragraph se-text-paragraph-align-center \" id=\"SE-a12df675-6d9f-4e25-8098-7497130494c3\" style=\"padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: 1.8; vertical-align: baseline; word-break: break-word; overflow-wrap: break-word;\"&gt; &lt;/p&gt; &lt;div class=\"autosourcing-stub-extra\" style=\"margin: 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; zoom: 1; opacity: 1;\"&gt; &lt;/div&gt; 꼭필요한 실무 엑셀 이 책과 함께 잘 공부해보셔요. &'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_reply = pd.DataFrame(book_reply)\n",
    "\n",
    "max_sen = book_reply.iloc[9399].item()\n",
    "\n",
    "max_sen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex로 전처리 테스트\n",
    "\n",
    "* +는 1개 이상 매치, * 0개 이상 매치되는 경우를 의미함. 이를 활용해 원하는 값을 추출할 수 있음.\n",
    "\n",
    "<br/>    \n",
    "\n",
    "\n",
    "```python \n",
    "\n",
    "re.findall('\\s*\\w*\\s*[\\u3130-\\u318F\\uAC00-\\uD7A3]+\\s*\\w*[.]*',max_sen)\n",
    "\n",
    "```\n",
    "\n",
    "* 위 regex를 풀어 설명하면 \n",
    "\n",
    "* \\s* : 0개 이상의 띄어쓰기를 가지고 있으며(띄어쓰기부터 시작해도 된다는 의미)\n",
    "* \\w* : 0개 이상의 영어 또는 숫자이며(영어부터 시작해도 된다는 의미)\n",
    "* [\\u3130-\\u318F\\uAC00-\\uD7A3]+ : 1개 이상의 한글이며(최소 1개 이상의 한글은 들어가야한다는 의미)\n",
    "* \\s* : 0개 이상의 띄어쓰기를 가지고 있으며 \n",
    "* \\w* : 0개 이상의 영어 또는 숫자이며\n",
    "* [.]* : 0개 이상의 마침표를 가진 경우를 \n",
    "* 모두 찾는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 사실 저같은 경우에는 수학적인 부분이 상당히 약하고 자신없어서 피할 수 있으면 최대한 피하고 살고 싶었는데요. 아이를 키우고 공부를 시키면서 수학이 우리 생활과 얼마나 밀접하게 연관이 되어 있는지를 알게 되었고요. 아이 문제풀이를 위해 엄마도 수학문제를 풀기 시작했고 무조건 거부하고 피하면 안되겠다는 생각이 들더라고요. 엑셀역시 마찬가지로 기본적인 것들은 할 수 있지만 아주 소소한 부분에 그치고 있고.. 진짜 제가 하고 싶고 배우고 싶은 것은 엑셀함수를 자유자재로 활용하여 유용하게 시간활용을 하고 효율적으로 일처리를 하는 것이서어ㅛ. 한정희 쌤의 책을 만나보게 되었는데 잘 선택했다는 생각이 들더라고요. 저도 구독완료 특히 이 책의 저자님은 유튜브에 짤막한 강좌를 업로드하신 터라 무료 동영상 160강도 함께 만나볼 수 있어 좋은데요. 책을 보니 책만으로도 충분히 훌륭하고 좋다는 생각이 들어서 엑셀을 공부할 때에 다른 곳에 한눈팔지 말고 이 책과 저자님의 동영상강의를 보면 되겠다 싶더라고요. 책의 앞쪽에는 서두르지 마라 그러나 멈추지도 마라.라는 요한 볼프강 폰 괴테의 명언이 적혀 있어서요. 다둥맘인지라 서두를 수 없는 저에게 뭔가 의미심장하게 다가왔는데.. 한 권 한 권 이지스퍼블리싱에서 만나보는 된다 시리즈들로 제 인생이 조금씩 긍정적인 방향으로 바뀌고 있지 않나 싶습니다. 이 책은 총 셋째 마당으로 이뤄져 있으며.. 첫째 마당 엑셀의 기본 데이터 입력과 관리 둘째 마당 엑셀의 꽃 수식과 함수 셋째 마당 보고서에 필수 데이터 집계와 시각화 의 순서로 엑셀정복을 하게 되는데요. 이 책에서는 바쁜 직장인을 위한 7일 속성 입문 코스는 물론.. 16일 코스 스케줄도 수록하고 있어서 원하시는대로 진도를 나가보시면 좋을 듯  7일 속성 입문 코스로는 1일 차  업무에 많이 쓰인느 양식 만들며 기본 익히기 2일 차 데이터베이스 관리 및 유효성 검사 3일 차엑셀의 꽃 수식과 함수의 기본 4일 차 IF함수와 VLOOKUP함수 5일 차 데이터 요약과 필터링 6일 차  차트 작성의 비밀 7일차  엑셀 끝판왕 피벗 의 순으로 진도를 나가시면 되는데요. 이렇게 7일 속성 입문 코스로 공부하신다면 하면 된다 위주로 실습을 진행하면 되시고요. 16일이면 책을 꼼꼼히 살펴볼 수 있어서 프로 엑셀러가 될 수 있다니.. 저는 16일 코스로 공부하려고용. 잠시 엑셀에 대해서 소개를 해드리면.. 엑셀은 마이크로소프트사에서 만들었으며 수힉과 함수를 사용하여 계산이 필요한 문서를 작성하는 프로그램인데요. 데이터베이스와 같은 많은 데이터를 요약하고 정리할 때에 정말 유용한데.. 비교하거나 강조할 데이터의 다지인 역시 자유자재로 바꿀 수 있어서요. 이를 바탕으로 하여 도표와 분석 차트도 쉽게 작성할 수 있다는 특징이 있답니다. 그리하여 직장에서는 기본적으로 엑셀을 참 많이 활용하게 되는 것 같은데요. 저 역시 잠시 머물던 회사에서 엑셀작업을 많이 해야했던 기억이 있는데.. 저보다 먼저 들어온 선배분이 슬며시 저에게 엑셀 사용방법을 물어보았던 것도 떠오릅니다. 사실 아랫사람에게 엑셀을 물어보는 것도 민망하고 누가 물어봤을 때 잘 알려주거나 대답하지 못하는 것도 민망하다죠. 직장생활을 하게 된다면 많이 사용하는 엑셀의 대표적인 내용들은 미리 알아두고 단축키도 암기해두면 유용할 듯 한데요. 이 책에서는 그러한 부분은 물론 그 이상을 담고 있다는 생각이 들더라고용. 요렇게 엑셀을 이용하여 견적서 양식도 만들 수 있고요. 각종 도표와 그래프 등도 만들 수 있으니 회의할 때도 넘 유용하겠더라고용. 사실 제가 이전에 했었던 엑셀은 엑셀이 아니라고 하고 싶은 것이.. 편리하게 사용한 것이 아니라 무식하게 입력하며 누구에게 말도 못하고 괴로워했던 것 같은데요. 수많은 데이터베이스를 엑셀의 장점을 활용하여 정리한다면 좋았을텐데 그때에 좀 더 준비를 했어야했다는 생각도 든다는.. 여전히 이런 부분을 잘하지 못하지만 첫술에 배부를 수가 있나요. 제 스스로 엑셀책을 찾아본다는 것이 하나의 큰 시도인 것 같은데요. 지금 당장 제가 피벗테이블을 이용하여 그럴싸한 보고용 차트를 만들 필요는 없지만 짬짬이 공부해볼 생각 아이들이 셋이나 있다보니 이런저런 걱정과 생각이 참 많은 저인데요. 걱정만 하지말고 회사에서 많이 쓰이는 실무엑셀을 잘 공부해둬서 나중에 어떠한 기회가 찾아온다면 잘 잡고 싶다는요. 그럼 저는 여기까지 이지스퍼블리싱 된다 7일 실무 엑셀 이었습니다. 꼭필요한 실무 엑셀 이 책과 함께 잘 공부해보셔요.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "re_result = re.findall('\\s*\\w*\\s*[\\u3130-\\u318F\\uAC00-\\uD7A3]+\\s*\\w*[.]*',max_sen)\n",
    "\n",
    "print(''.join(re_result))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas를 이용해 문장 전체를 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    [점프 투 파이썬,  책의 커리큘럼처럼 이어져서,  좋다.]\n",
      "1    [다만,  사실상 거의 입문,  표준인 파이참이나 vscode,  로 에디터를 사용...\n",
      "Name: 0, dtype: object\n",
      "0                          점프 투 파이썬 책의 커리큘럼처럼 이어져서 좋다.\n",
      "1    다만 사실상 거의 입문 표준인 파이참이나 vscode 로 에디터를 사용했으면 좋았겠...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "regex_result = book_reply[0].str.findall('\\s*\\w*\\s*[\\u3130-\\u318F\\uAC00-\\uD7A3]+\\s*\\w*[.]*')\n",
    "print(regex_result[:2])\n",
    "\n",
    "regex_result = regex_result.apply(lambda x : ''.join(x))\n",
    "print(regex_result[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장별 길이 재확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9399     2190\n",
       "92987    1699\n",
       "99430    1515\n",
       "48162    1090\n",
       "66554    1029\n",
       "         ... \n",
       "93477       0\n",
       "15525       0\n",
       "63215       0\n",
       "15498       0\n",
       "93022       0\n",
       "Name: 0, Length: 102367, dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_result_len = regex_result.str.len().sort_values(ascending=False)\n",
    "\n",
    "regex_result_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 길이를 가진 문장 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 사실 저같은 경우에는 수학적인 부분이 상당히 약하고 자신없어서 피할 수 있으면 최대한 피하고 살고 싶었는데요. 아이를 키우고 공부를 시키면서 수학이 우리 생활과 얼마나 밀접하게 연관이 되어 있는지를 알게 되었고요. 아이 문제풀이를 위해 엄마도 수학문제를 풀기 시작했고 무조건 거부하고 피하면 안되겠다는 생각이 들더라고요. 엑셀역시 마찬가지로 기본적인 것들은 할 수 있지만 아주 소소한 부분에 그치고 있고.. 진짜 제가 하고 싶고 배우고 싶은 것은 엑셀함수를 자유자재로 활용하여 유용하게 시간활용을 하고 효율적으로 일처리를 하는 것이서어ㅛ. 한정희 쌤의 책을 만나보게 되었는데 잘 선택했다는 생각이 들더라고요. 저도 구독완료 특히 이 책의 저자님은 유튜브에 짤막한 강좌를 업로드하신 터라 무료 동영상 160강도 함께 만나볼 수 있어 좋은데요. 책을 보니 책만으로도 충분히 훌륭하고 좋다는 생각이 들어서 엑셀을 공부할 때에 다른 곳에 한눈팔지 말고 이 책과 저자님의 동영상강의를 보면 되겠다 싶더라고요. 책의 앞쪽에는 서두르지 마라 그러나 멈추지도 마라.라는 요한 볼프강 폰 괴테의 명언이 적혀 있어서요. 다둥맘인지라 서두를 수 없는 저에게 뭔가 의미심장하게 다가왔는데.. 한 권 한 권 이지스퍼블리싱에서 만나보는 된다 시리즈들로 제 인생이 조금씩 긍정적인 방향으로 바뀌고 있지 않나 싶습니다. 이 책은 총 셋째 마당으로 이뤄져 있으며.. 첫째 마당 엑셀의 기본 데이터 입력과 관리 둘째 마당 엑셀의 꽃 수식과 함수 셋째 마당 보고서에 필수 데이터 집계와 시각화 의 순서로 엑셀정복을 하게 되는데요. 이 책에서는 바쁜 직장인을 위한 7일 속성 입문 코스는 물론.. 16일 코스 스케줄도 수록하고 있어서 원하시는대로 진도를 나가보시면 좋을 듯  7일 속성 입문 코스로는 1일 차  업무에 많이 쓰인느 양식 만들며 기본 익히기 2일 차 데이터베이스 관리 및 유효성 검사 3일 차엑셀의 꽃 수식과 함수의 기본 4일 차 IF함수와 VLOOKUP함수 5일 차 데이터 요약과 필터링 6일 차  차트 작성의 비밀 7일차  엑셀 끝판왕 피벗 의 순으로 진도를 나가시면 되는데요. 이렇게 7일 속성 입문 코스로 공부하신다면 하면 된다 위주로 실습을 진행하면 되시고요. 16일이면 책을 꼼꼼히 살펴볼 수 있어서 프로 엑셀러가 될 수 있다니.. 저는 16일 코스로 공부하려고용. 잠시 엑셀에 대해서 소개를 해드리면.. 엑셀은 마이크로소프트사에서 만들었으며 수힉과 함수를 사용하여 계산이 필요한 문서를 작성하는 프로그램인데요. 데이터베이스와 같은 많은 데이터를 요약하고 정리할 때에 정말 유용한데.. 비교하거나 강조할 데이터의 다지인 역시 자유자재로 바꿀 수 있어서요. 이를 바탕으로 하여 도표와 분석 차트도 쉽게 작성할 수 있다는 특징이 있답니다. 그리하여 직장에서는 기본적으로 엑셀을 참 많이 활용하게 되는 것 같은데요. 저 역시 잠시 머물던 회사에서 엑셀작업을 많이 해야했던 기억이 있는데.. 저보다 먼저 들어온 선배분이 슬며시 저에게 엑셀 사용방법을 물어보았던 것도 떠오릅니다. 사실 아랫사람에게 엑셀을 물어보는 것도 민망하고 누가 물어봤을 때 잘 알려주거나 대답하지 못하는 것도 민망하다죠. 직장생활을 하게 된다면 많이 사용하는 엑셀의 대표적인 내용들은 미리 알아두고 단축키도 암기해두면 유용할 듯 한데요. 이 책에서는 그러한 부분은 물론 그 이상을 담고 있다는 생각이 들더라고용. 요렇게 엑셀을 이용하여 견적서 양식도 만들 수 있고요. 각종 도표와 그래프 등도 만들 수 있으니 회의할 때도 넘 유용하겠더라고용. 사실 제가 이전에 했었던 엑셀은 엑셀이 아니라고 하고 싶은 것이.. 편리하게 사용한 것이 아니라 무식하게 입력하며 누구에게 말도 못하고 괴로워했던 것 같은데요. 수많은 데이터베이스를 엑셀의 장점을 활용하여 정리한다면 좋았을텐데 그때에 좀 더 준비를 했어야했다는 생각도 든다는.. 여전히 이런 부분을 잘하지 못하지만 첫술에 배부를 수가 있나요. 제 스스로 엑셀책을 찾아본다는 것이 하나의 큰 시도인 것 같은데요. 지금 당장 제가 피벗테이블을 이용하여 그럴싸한 보고용 차트를 만들 필요는 없지만 짬짬이 공부해볼 생각 아이들이 셋이나 있다보니 이런저런 걱정과 생각이 참 많은 저인데요. 걱정만 하지말고 회사에서 많이 쓰이는 실무엑셀을 잘 공부해둬서 나중에 어떠한 기회가 찾아온다면 잘 잡고 싶다는요. 그럼 저는 여기까지 이지스퍼블리싱 된다 7일 실무 엑셀 이었습니다. 꼭필요한 실무 엑셀 이 책과 함께 잘 공부해보셔요.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_result.iloc[9399]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 Token 개수를 줄이기 위해 문장 나누기\n",
    "\n",
    "* kiwi 라이브러리 활용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [(점프 투 파이썬 책의 커리큘럼처럼 이어져서 좋다., 0, 27, None, [])]\n",
       "1         [(다만 사실상 거의 입문 표준인 파이참이나 vscode 로 에디터를 사용했으면 좋...\n",
       "2                  [(입문자들은 그냥 다 따라서 하니까., 0, 19, None, [])]\n",
       "3                [(파이썬이 사용되는 예제들이 많아 좋아요, 0, 21, None, [])]\n",
       "4                      [(파이썬 라이브러리가 방대하다, 0, 15, None, [])]\n",
       "                                ...                        \n",
       "102362                  [(사진가에게 적합한 책입니다, 0, 14, None, [])]\n",
       "102363                      [(기존에 봤던 책.., 0, 10, None, [])]\n",
       "102364                 [(내용이 좋아 선물용으로 구입, 0, 15, None, [])]\n",
       "102365                 [(정말 유용한 내용이 많습니다, 0, 15, None, [])]\n",
       "102366          [(포토샵 배우기에 최고의 책인거 같습니다., 0, 22, None, [])]\n",
       "Name: 0, Length: 102367, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "sep_result = regex_result.apply(lambda x : kiwi.split_into_sents(x))\n",
    "\n",
    "new_sep_result = []\n",
    "for i in sep_result.tolist() :\n",
    "    for j in i :\n",
    "        new_sep_result.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 문장 : 101787개\n"
     ]
    }
   ],
   "source": [
    "df_sep_result = pd.DataFrame(new_sep_result)\n",
    "\n",
    "print(f'전처리 문장 : {len(df_sep_result)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sep_result = df_sep_result[0].str.replace('HTML5','HTML')\n",
    "# df_sep_result = df_sep_result.str.replace('ES6','js')\n",
    "# df_sep_result = df_sep_result.str.replace('ggplot2','ggplot')\n",
    "# df_sep_result = df_sep_result.str.replace('[a-zA-Z]+[0-9]+','',regex=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장길이 5 이상인 경우 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          점프 투 파이썬 책의 커리큘럼처럼 이어져서 좋다.\n",
       "1    다만 사실상 거의 입문 표준인 파이참이나 vscode 로 에디터를 사용했으면 좋았겠...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sep_result = df_sep_result[df_sep_result.str.len() > 5]\n",
    "\n",
    "df_sep_result = df_sep_result.reset_index(drop=True)\n",
    "\n",
    "df_sep_result[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as keu\n",
    "\n",
    "## Eng to han\n",
    "englist = pd.read_csv(\"preprocess/englist.csv\")\n",
    "\n",
    "lst = []\n",
    "for i in df_sep_result[0].tolist() :\n",
    "    val = keu.trans_eng_to_han(words=i,englist=englist)\n",
    "\n",
    "    lst.append(' '.join(val))\n",
    "\n",
    "\n",
    "df_sep_result = pd.DataFrame(lst)\n",
    "\n",
    "# len 5개 이상만 포함\n",
    "df_sep_result = df_sep_result[df_sep_result[0].str.len() > 5]\n",
    "\n",
    "\n",
    "df_sep_result.to_csv('pre_book_reply.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 개수 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token 개수 세기\n",
    "tokens = tokenizer(df_sep_result['0'].tolist())\n",
    "\n",
    "# token, len(token)\n",
    "token = [[token,len(token)] for token in tokens['input_ids']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 토큰개수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>num</th>\n",
       "      <th>sen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 28, 14370, 4279, 4031, 9577, 8800, 9108, 2...</td>\n",
       "      <td>250</td>\n",
       "      <td>8 할당하기 Chapter 9 태그 라벨 계정 Chapter 10 목표 달성을 위한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2634, 4112, 31727, 2399, 4568, 6517, 12504...</td>\n",
       "      <td>248</td>\n",
       "      <td>맑은 고딕 돋움 일반 응용프로그램을 포함한 많은 개발자 Apple맑은 고딕 돋움 대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 9246, 17222, 6827, 13606, 4031, 9246, 4418...</td>\n",
       "      <td>243</td>\n",
       "      <td>실무 함수 제대로 익히기 실무함수 01 꼭 알아야 할 필수 함수 01 목표갑과 매출...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token  num  \\\n",
       "0  [2, 28, 14370, 4279, 4031, 9577, 8800, 9108, 2...  250   \n",
       "1  [2, 2634, 4112, 31727, 2399, 4568, 6517, 12504...  248   \n",
       "2  [2, 9246, 17222, 6827, 13606, 4031, 9246, 4418...  243   \n",
       "\n",
       "                                                 sen  \n",
       "0  8 할당하기 Chapter 9 태그 라벨 계정 Chapter 10 목표 달성을 위한...  \n",
       "1  맑은 고딕 돋움 일반 응용프로그램을 포함한 많은 개발자 Apple맑은 고딕 돋움 대...  \n",
       "2  실무 함수 제대로 익히기 실무함수 01 꼭 알아야 할 필수 함수 01 목표갑과 매출...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token = pd.DataFrame(token)\n",
    "\n",
    "# sen 추가\n",
    "df_token['sen'] = df_sep_result['0'].tolist()\n",
    "\n",
    "df_token.columns = ['token','num','sen']\n",
    "\n",
    "\n",
    "# 3 이상 256 이하 token 개수만 남기기\n",
    "df_token = df_token[(df_token.num < 256) & (df_token.num > 3) ].sort_values(by='num',ascending=False)\n",
    "\n",
    "df_token = df_token.reset_index(drop=True)\n",
    "\n",
    "df_token.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   파일저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sep_result.to_csv('pre_book_reply.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator와 Discriminator 학습을 위한 Optimizer 설정 방법\n",
    "\n",
    "* Electra는 combined loss를 활용해 Generator와 Discriminator를 학습함.\n",
    "\n",
    "* combined loss는 Generator loss($\\mathcal{L}_{\\text{MLM}}(\\textbf{x}, \\theta_G)$)와 Discriminator loss($ \\mathcal{L}_{Disc} (\\textbf{x}, \\theta_{D})$)를 각각 계산한 뒤 아래의 식으로 구함.\n",
    "\n",
    "\n",
    "    combined_loss = $ \\min_{\\theta_G, \\theta_D} \\sum_{\\textbf{x} \\in \\mathcal{X}} \\mathcal{L}_{\\text{MLM}}(\\textbf{x}, \\theta_G) + \\lambda \\mathcal{L}_{Disc} (\\textbf{x}, \\theta_{D})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformers에서 `save_pretrained` 매서드 사용시 학습된 모델의 weight 저장이 안되는 문제 식별\n",
    "\n",
    "* 새로 학습한 모델을 저장한 모델의 예측 결과와 학습 이전의 원본 모델의 예측 결과가 동일함.\n",
    "\n",
    "* 학습이 제대로 수행되지 않는 문제인지, 저장 방법의 문제인지 파악 필요\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습한 모델(trained)와 기존 모델(old) 간 output 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "import torch\n",
    "\n",
    "raw_data = pd.read_csv('data/pre_book_total_128.csv')\n",
    "\n",
    "token = ast.literal_eval(raw_data.loc[0,'token'])\n",
    "\n",
    "# 예제 input\n",
    "input_data = torch.tensor(token).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel\n",
    "\n",
    "# 모델 불러오기\n",
    "old = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "trained = ElectraModel.from_pretrained('model/discriminator-iter2/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1475, -0.4093, -0.3794,  ..., -0.1255, -0.0811, -0.2957],\n",
       "         [ 0.2833, -0.3376, -0.4579,  ...,  0.1145,  0.5214, -0.2951],\n",
       "         [ 1.2557, -0.1452, -0.4150,  ..., -0.1571,  0.1779, -0.0560],\n",
       "         ...,\n",
       "         [ 0.0982, -0.2955, -0.6046,  ..., -0.3594,  0.1047, -0.0694],\n",
       "         [-0.4510,  0.1381, -0.6813,  ...,  0.3969,  0.1989, -0.0629],\n",
       "         [-0.1475, -0.4093, -0.3794,  ..., -0.1255, -0.0811, -0.2957]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_one = old(input_data).last_hidden_state\n",
    "trained = trained(input_data).last_hidden_state\n",
    "\n",
    "old_one == trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "pip install transformers\n",
    "\n",
    "import os\n",
    "!pwd\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/electra_for_fine_tuning')\n",
    "!pwd\n",
    "\n",
    "import pandas as pd\n",
    "import ast \n",
    "\n",
    "raw_data = pd.read_csv('data/pre_book_total_128.csv')\n",
    "\n",
    "token = ast.literal_eval(raw_data.loc[0,'token'])\n",
    "\n",
    "# 예제 input\n",
    "input_data = torch.tensor(token).reshape(1,-1).to(device)\n",
    "\n",
    "from transformers import ElectraModel\n",
    "\n",
    "# 모델 불러오기\n",
    "old = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator').to(device)\n",
    "trained = discriminator.electra\n",
    "\n",
    "\n",
    "old_one = old(input_data).last_hidden_state\n",
    "trained = trained(input_data).last_hidden_state\n",
    "\n",
    "old_one == trained"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 접근 과정\n",
    "\n",
    "* 학습 과정에 대한 문제라면 몇 번을 학습하더라도 기존 모델의 output과 동일할 것임.\n",
    "\n",
    "* 이러한 문제가 아니라면 batch를 1회라도 학습한 모델의 output은 기존 모델의 output과 다를 것임\n",
    "\n",
    "\n",
    "* 실험 결과 : batch_size 16으로 100번 학습한 모델의 output은 기존 모델의 output과 다름.\n",
    "``` python\n",
    "tensor([[[False, False, False,  ..., False, False, False],\n",
    "         [False, False, False,  ..., False, False, False],\n",
    "         [False, False, False,  ..., False, False, False],\n",
    "         ...,\n",
    "         [False, False, False,  ..., False, False, False],\n",
    "         [False, False, False,  ..., False, False, False],\n",
    "         [False, False, False,  ..., False, False, False]]])\n",
    "```\n",
    "* 결론 : 모델을 저장하는 과정에서 학습된 모델을 저장하지 않아서 발생한 문제임.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장 과정에서 발생하는 문제 식별\n",
    "\n",
    "* 기존 모델(old)의 weight와 새로 학습한 모델(new)의 weight가 동일한 문제 식별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel\n",
    "\n",
    "# 모델 불러오기\n",
    "old = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator') #len() = 197개\n",
    "new = ElectraModel.from_pretrained('model/discriminator-iter2/') #len() = 197개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_param = [i for i in old.parameters()]\n",
    "new_param = [i for i in new.parameters()]\n",
    "\n",
    "# Params 비교\n",
    "idx_list = []\n",
    "for i in range(len(old_param)) :\n",
    "    x = old_param[i] - new_param[i] # 차이가 없다면 x = 0\n",
    "    if torch.sum(x.reshape(-1)).item() != 0 :\n",
    "        idx_list.append(i)\n",
    "\n",
    "idx_list \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 저장하는 매서드(`save_pretrained`)가 문제라 가정하고 접근\n",
    "\n",
    "* huggingface는 torch 기반이므로 torch.save로 모델 저장 가능\n",
    "\n",
    "* torch로 저장한 모듈을 불러와 params 비교 수행\n",
    "\n",
    "* 결론 : `save_pretrained` 매서드가 문제임을 발견"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved_by_torch = torch.load('123.pt')\n",
    "old = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator') #len() = 197개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]\n"
     ]
    }
   ],
   "source": [
    "old_param = [i for i in old.parameters()]\n",
    "new_param = [i for i in model_saved_by_torch.parameters()]\n",
    "\n",
    "idx_list = []\n",
    "for i in range(len(old_param)) :\n",
    "    x = old_param[i] - new_param[i]\n",
    "    if torch.sum(x.reshape(-1)).item() != 0 :\n",
    "        # print(torch.sum(x.reshape(-1)).item())\n",
    "        idx_list.append(i)\n",
    "\n",
    "## 모든 param가 학습 됨을 확인함.\n",
    "print(idx_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.load로 불러온 모델에 다시 save_pretrained로 저장\n",
    "* torch 파일로 저장한 모델의 경우 weight를 transformers 모델에 저장할 수 있음.\n",
    "* pre-training 과정에서는 torch로 저장한 다음 최종적으로 save_pretrained으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved_by_torch = ElectraModel.from_pretrained('123') # torch로 불러온 뒤 save_pretrained로 저장한 파일\n",
    "old = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator') #len() = 197개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]\n"
     ]
    }
   ],
   "source": [
    "old_param = [i for i in old.parameters()]\n",
    "new_param = [i for i in model_saved_by_torch.parameters()]\n",
    "\n",
    "idx_list = []\n",
    "for i in range(len(old_param)) :\n",
    "    x = old_param[i] - new_param[i]\n",
    "    if torch.sum(x.reshape(-1)).item() != 0 :\n",
    "        # print(torch.sum(x.reshape(-1)).item())\n",
    "        idx_list.append(i)\n",
    "\n",
    "## 모든 param가 학습 됨을 확인함.\n",
    "print(idx_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
