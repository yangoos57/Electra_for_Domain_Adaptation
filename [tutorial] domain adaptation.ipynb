{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tutorial] Electra Domain Adaptation Tutorial With Huggingface\n",
    "\n",
    "### 참고사항\n",
    "* 단계별 상세 설명은 [Huggingface로 ELECTRA 학습하기 : Domain Adaptation](https://yangoos57.github.io/blog/DeepLearning/paper/Electra/electra/) 참고\n",
    "\n",
    "* 구동환경\n",
    "\n",
    "  ```python\n",
    "    torch == 1.12.1\n",
    "    pandas == 1.4.3\n",
    "    transformers == 4.20.1\n",
    "    datasets == 2.8.0\n",
    "  ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Electra Model 불러오기\n",
    "\n",
    "* Generator는 ElectraForMaskedLM로 불러오고, Descriminator은 ElectraForPreTraining로 불러와야함.\n",
    "\n",
    "* ElectraForMaskedLM는 Mask 토큰에 들어갈 단어를 예측하는 기능, ElectraForPreTraining는 문장 내 토큰의 진위여부를 판별하는 기능을 수행함.\n",
    "\n",
    "* [monologg님의 KoELECTRA](https://github.com/monologg/KoELECTRA)를 베이스 모델로 활용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizer, ElectraForMaskedLM\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "\n",
    "generator = ElectraForMaskedLM.from_pretrained('monologg/koelectra-base-v3-generator')\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Huggingface의 Datasets 라이브러리로 데이터 불러오기\n",
    "* Huggingface의 Trainer로 모델을 학습할 예정이라면 Datasets으로 학습 자료를 불러오는 것을 추천\n",
    "\n",
    "* pytorch의 Dataset으로 Trainer를 사용할 수 있으나 경험 상 디버깅이 상당히 번거로움.\n",
    "* Trainer와 연동성이 보장된 Dataset은 간편하게 데이터를 활용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5b33c921347e3fef\n",
      "Found cached dataset csv (/Users/yangwoolee/.cache/huggingface/datasets/csv/default-5b33c921347e3fef/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ed017a1a3f41f19a1c6554763167f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-293d316a8dadeefa\n",
      "Found cached dataset csv (/Users/yangwoolee/.cache/huggingface/datasets/csv/default-293d316a8dadeefa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c5c4e149984cc18b0276e71f97dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset('csv',data_files='data/book_train_128.csv')\n",
    "validation = load_dataset('csv',data_files='data/book_validation_128.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 토크나이징\n",
    "* Trainer에 활용하기 위해선 데이터에 대한 토크나이징을 수행해야함.\n",
    "\n",
    "* Datasets에서 제공하는 map 함수를 활용하면 간편하게 토크나이징이 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-5b33c921347e3fef/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fa3f38cc1a560dd0.arrow\n",
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-293d316a8dadeefa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e2ba2932d368e7cc.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sen'], max_length=128, padding=True, truncation=True)\n",
    "\n",
    "train_data_set = train['train'].map(tokenize_function)\n",
    "validation_data_set = validation['train'].map(tokenize_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generator와 Descriminator 학습을 위한 Electra Model 설계\n",
    "\n",
    "- Electra-pytorch 라이브러리를 Transformers 라이브러리에서 활용할 수 있도록 일부 수정하였음.\n",
    "\n",
    "- Electra-pytorh 원본 github 주소 : https://github.com/lucidrains/electra-pytorch\n",
    "\n",
    "- 해당 모델은 아래의 3단계를 수행하기 위해 설계되었음\n",
    "    - 1단계 : input data masking\n",
    "    - 2단계 : Generator 학습 및 fake sentence 생성\n",
    "    - 3단계 : Discriminator 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple(\n",
    "    \"Results\",\n",
    "    [\n",
    "        \"loss\",\n",
    "        \"mlm_loss\",\n",
    "        \"disc_loss\",\n",
    "        \"gen_acc\",\n",
    "        \"disc_acc\",\n",
    "        \"disc_labels\",\n",
    "        \"disc_predictions\",\n",
    "        \"origin\",\n",
    "        \"disc\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 모델 내부에서 활용되는 함수 정의\n",
    "\n",
    "\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "\n",
    "def gumbel_sample(t, temperature=1.0):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "\n",
    "# main electra class\n",
    "\n",
    "\n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        tokenizer,\n",
    "        *,\n",
    "        num_tokens=35000,\n",
    "        mask_prob=0.15,\n",
    "        replace_prob=0.85,\n",
    "        mask_token_id=4,\n",
    "        pad_token_id=0,\n",
    "        mask_ignore_token_ids=[2, 3],\n",
    "        disc_weight=50.0,\n",
    "        gen_weight=1.0,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        num_tokens: 모델 vocab_size\n",
    "        mask_prob: 토큰 중 [MASK] 토큰으로 대체되는 비율\n",
    "        replace_prop:  토큰 중 [MASK] 토큰으로 대체되는 비율(?????)\n",
    "        mask_token_i: [MASK] Token id\n",
    "        pad_token_i: [PAD] Token id\n",
    "        mask_ignore_token_id: [CLS],[SEP] Token id\n",
    "        disc_weigh: discriminator loss의 Weight 조정을 위한 값\n",
    "        gen_weigh: generator loss의 Weight 조정을 위한 값\n",
    "        temperature: gumbel_distribution에 활용되는 arg, 값이 높을수록 모집단 분포와 유사한 sampling 수행\n",
    "        \"\"\"\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "\n",
    "        try:\n",
    "            input = input_ids[\"input_ids\"]\n",
    "        except:\n",
    "            input = input_ids\n",
    "\n",
    "        # ------ 1단계 Input Data Masking --------#\n",
    "\n",
    "        \"\"\"\n",
    "        - Generator는 Bert와 구조도 동일하고 학습하는 방법도 동일함. \n",
    "\n",
    "        - Generator 학습을 위해선 [Masked] 토큰이 필요하므로 input data를 Masking하는 과정이 필요함.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(\n",
    "            masking_mask * replace_prob, self.mask_token_id\n",
    "        )\n",
    "\n",
    "        # ------ 2단계 Masking 된 문장을 Generator가 학습하고 가짜 Token을 생성 --------#\n",
    "\n",
    "        \"\"\"\n",
    "        - Generator를 학습하여 MLM_loss 계산(combined_loss 계산에 활용)\n",
    "        - Generator에서 예측한 문장을 Discriminator 학습에 활용\n",
    "        - ex) 원본 문장 :  특히 안드로이드 플랫폼 기반의 (웹)앱과 (하이)브드리앱에 초점을 맞추고 있다\n",
    "              가짜 문장 :  특히 안드로이드 플랫폼 기반의 (마이크로)앱과 (이)브드리앱에 초점을 맞추고 있다\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # get generator output and get mlm loss(수정)\n",
    "        logits = self.generator(masked_input, **kwargs).logits\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2), gen_labels, ignore_index=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature=self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # ------ 3단계 가짜 Token의 진위여부를 Discriminator가 판단하는 단계 --------#\n",
    "\n",
    "        \"\"\"\n",
    "        - 가짜 문장을 학습해 개별 토큰에 대해 진위여부를 판단\n",
    "        - 진짜 token이라 판단하면 0, 가짜 토큰이라 판단하면 1을 부여\n",
    "        - 정답과 비교해 disc_loss를 계산(combined_loss 계산에 활용)\n",
    "        - combined_loss : 학습의 최종 loss임. 모델은 combined_loss의 최솟값을 얻기 위한 방식으로 학습 진행\n",
    "        \"\"\"\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator(disc_input, **kwargs).logits\n",
    "        disc_logits_reshape = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits_reshape[non_padded_indices], disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # combined loss 계산\n",
    "        # disc_weight을 50으로 주는 이유는 discriminator의 task가 복잡하지 않기 떄문임.\n",
    "        # mlm loss의 경우 vocab_size(=35000) 만큼의 loos 계산을 수행하지만\n",
    "        # disc_loss의 경우 src_token_len 만큼의 loss 계산을 수행한만큼\n",
    "        # loss 값에 큰 차이가 발생함. disc_weight은 이를 보완하는 weight임.\n",
    "        combined_loss = self.gen_weight * mlm_loss + self.disc_weight * disc_loss\n",
    "\n",
    "        # ------ 모델 성능 및 학습 과정을 추적하기 위한 지표(Metrics) 설계 --------#\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # gen mask 예측\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # fake token 진위 예측\n",
    "            disc_predictions = torch.round(\n",
    "                (torch.sign(disc_logits_reshape) + 1.0) * 0.5\n",
    "            )\n",
    "            # generator_accuracy\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "\n",
    "            # discriminator_accuracy\n",
    "            disc_acc = (\n",
    "                0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean()\n",
    "                + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "            )\n",
    "\n",
    "        return Results(\n",
    "            combined_loss,\n",
    "            mlm_loss,\n",
    "            disc_loss,\n",
    "            gen_acc,\n",
    "            disc_acc,\n",
    "            disc_labels,\n",
    "            disc_predictions,\n",
    "            input,\n",
    "            disc_input,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "model = Electra(generator=generator,discriminator=discriminator,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Trainer 기타 기능 설정 및 학습\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✓ 훈련 옵션 설정(선택사항)\n",
    "\n",
    "* 훈련에 사용되는 모든 arguments를 `TrainingArguments`를 통해 조정할 수 있음\n",
    "\n",
    "* `logging_stetps`는 {loss,learning_rate,epoch} 정보를 몇번의 step 간격으로 수행해야할지 설정\n",
    "* `evaluation_strategy`는 training 중 evaluation을 어느 때 실행해야할지 설정 'epoch'와 'step'이 있음. evaluation_strategy를 설정하지 않으면 학습 중 evaluation을 진행하지 않음.\n",
    "\n",
    "\n",
    "#### ✓ Input data 가공을 위한 Data collater 설정\n",
    "* Data callter은 학습 목적에 맞게 input data를 가공하는 방법을 설정\n",
    "\n",
    "* `DataCollatorForLanguageModeling`는 Input_data에 [MASK]를 포함하도록 가공하는 collater임. 따라서 Bert 모델 학습에 필히 설정해야함.\n",
    "\n",
    "* Transformers는 `DataCollatorForLanguageModeling` 외에도 여러 학습 방법에 맞게 데이터를 가공하는 collater를 제공 (`DataCollatorWithPadding`, `DataCollatorForTokenClassification` 등)\n",
    "\n",
    "\n",
    "\n",
    "#### ✓ Callback 정의하기(선택사항)\n",
    "\n",
    "> callback에 대한 상세한 설명은 ____ 참고\n",
    "\n",
    "* callback은 학습 중 Trainer가 추가로 수행해야하는 Task를 정의함.\n",
    "\n",
    "* 미리 정의된 callback을 사용하거나 아래 코드와 같이 커스텀하여 사용할 수 있음\n",
    "\n",
    "* 아래의 `myCallback`은 100번째 Step마다 현재 epoch와 step을 출력하는 Task를 정의함.\n",
    "\n",
    "\n",
    "#### ✓ Custom Trainer 만들기(선택사항)\n",
    "\n",
    "* Trainer 내부 함수를 목적에 맞게 변경할 수 있음.\n",
    "\n",
    "* Trainer를 커스터마이징하면 아래의 예시처럼 모델 학습 경과를 시각화 할 수 있음.\n",
    "\n",
    "```python \n",
    "    0번째 epoch 진행 중 ------- 20번째 step 결과\n",
    "    input 문장 : [MASK]이 출간된지 꽤 됬다고 생각하는데 실습하는데 전혀 [MASK]없습니다\n",
    "    output 문장 : [책]이 출간된지 꽤 됬다고 생각하는데 실습하는데 전혀 [문제]없습니다\n",
    "```\n",
    "\n",
    "* Trainer 내부의 `compute_loss` 함수를 활용하면 input_data와 모델 학습 결과인 output_data에 접근할 수 있음\n",
    "\n",
    "> 해당 매서드를 callback으로 구현하기에는 callback이 input_data와 output_data에 접근하기 까다롭기 때문에 \n",
    ">\n",
    "> Trainer를 커스터마이징 하는 방법을 추천\n",
    "\n",
    "\n",
    "#### ✓ Trainer 정의 및 학습 시작\n",
    "\n",
    "* 지금까지 설정한 옵션, 데이터셋을 Trainer의 args로 활용\n",
    "\n",
    "* 이후 train() 매서드를 통해 학습 시작\n",
    "\n",
    "* Trainer는 매 500회 step 이후 학습된 모델을 저장하며, 학습이 중간에 중단되더라도 trainer('폴더 경로')를 통해 중단된 부분부터 새롭게 학습이 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `Electra.forward` and have been ignored: Unnamed: 0, sen, attention_mask, token_type_ids. If Unnamed: 0, sen, attention_mask, token_type_ids are not expected by `Electra.forward`,  you can safely ignore this message.\n",
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219bca7663a24a8e8c413b756af71b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0번째 epoch 진행 중 ------- 0번째 step 결과\n",
      "\n",
      "\n",
      "원본 문장 :  국내 TOP 기업 임직원 강력 추천 화제의 서울대 데이터마이닝캠프 프로그램 수록 데이터 분석 최신 (경향) 및 사례 총정리대한민국 인공지능 빅데이터 (분야)를 이끄는 (##조) (##성)준 서울대 교수와 국내 최고 석학들의 (##절)대 (실패)하지 않는 (실전) 데이터 분석 (##법) 대공개 왜 어떤 사람은 데이터로 성공하고 어떤 사람은 실패하는가 정답을 찾고 싶은 당신에게 필요한 특별한 빅데이터 강의지금 우리는 일상이 데이터가 되는 시대를 넘어 데이터가 일상이 되는 시대를 살고 있다\n",
      "가짜 문장 :  국내 TOP 기업 임직원 강력 추천 화제의 서울대 데이터마이닝캠프 프로그램 수록 데이터 분석 최신 (자료) 및 사례 총정리대한민국 인공지능 빅데이터 (역사)를 이끄는 (수) (김학)준 서울대 교수와 국내 최고 석학들의 (모두)대 (##담)하지 않는 (메르스) 데이터 분석 (기술) 대공개 왜 어떤 사람은 데이터로 성공하고 어떤 사람은 실패하는가 정답을 찾고 싶은 당신에게 필요한 특별한 빅데이터 강의지금 우리는 일상이 데이터가 되는 시대를 넘어 데이터가 일상이 되는 시대를 살고 있다\n",
      "\n",
      "\n",
      "118개 토큰 중 6개 예측 실패 -------- 8개 가짜 토큰 중 3개 판별\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장 위치</th>\n",
       "      <th>실제 토큰</th>\n",
       "      <th>(가짜)토큰</th>\n",
       "      <th>실제</th>\n",
       "      <th>예측</th>\n",
       "      <th>정답</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>##조</td>\n",
       "      <td>(수)</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>##절</td>\n",
       "      <td>(모두)</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>실전</td>\n",
       "      <td>(메르스)</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>경향</td>\n",
       "      <td>(자료)</td>\n",
       "      <td>fake</td>\n",
       "      <td>-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>분야</td>\n",
       "      <td>(역사)</td>\n",
       "      <td>fake</td>\n",
       "      <td>-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>##성</td>\n",
       "      <td>(김학)</td>\n",
       "      <td>fake</td>\n",
       "      <td>-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48</td>\n",
       "      <td>실패</td>\n",
       "      <td>(##담)</td>\n",
       "      <td>fake</td>\n",
       "      <td>-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56</td>\n",
       "      <td>##법</td>\n",
       "      <td>(기술)</td>\n",
       "      <td>fake</td>\n",
       "      <td>-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>인공지능</td>\n",
       "      <td>인공지능</td>\n",
       "      <td>-</td>\n",
       "      <td>fake</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss 12.756 -- Generator Loss : 2.734 -- Discriminator Loss : 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.4502, 'train_samples_per_second': 2.432, 'train_steps_per_second': 0.243, 'train_loss': 10.733702659606934, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=10.733702659606934, metrics={'train_runtime': 16.4502, 'train_samples_per_second': 2.432, 'train_steps_per_second': 0.243, 'train_loss': 10.733702659606934, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, TrainerCallback,Trainer,DataCollatorForLanguageModeling\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    # evaluation_strategy='epoch'\n",
    "    )\n",
    "\n",
    "class myCallback(TrainerCallback):\n",
    "\n",
    "    def on_step_begin(self, args, state, control, logs=None, **kwargs):\n",
    "\n",
    "        # 함수 이름.. 언제 시작할지\n",
    "        # log는 설정할 때마다\n",
    "        # arg,state,control은 참고할 수 있는 attribute의 경우임.\n",
    "        # 근데 내가 필요한건 input\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            print(\"\")\n",
    "            print(\n",
    "                f\"{int(state.epoch)}번째 epoch 진행 중 ------- {state.global_step}번째 step 결과\"\n",
    "            )\n",
    "\n",
    "\n",
    "class customtrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    ############# 내용 추가\n",
    "    def step_check(self):\n",
    "        # state는 현 상태를 담는 attribute임.\n",
    "        return self.state.global_step\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        # ############# 내용 추가\n",
    "        if self.step_check() % self.args.logging_steps == 0:\n",
    "            with torch.no_grad() :\n",
    "                \n",
    "                origin_id = outputs.origin[0].tolist()\n",
    "                pad_idx = origin_id.index(0) if 0 in origin_id else None\n",
    "\n",
    "                origin_id = origin_id[:pad_idx]\n",
    "                disc_id = outputs.disc[0].tolist()[:pad_idx]\n",
    "\n",
    "                origin_tokens = tokenizer.convert_ids_to_tokens(origin_id)\n",
    "                \n",
    "                # print 용도 \n",
    "                origin_tokens_for_print = origin_tokens.copy()\n",
    "\n",
    "                disc_tokens = tokenizer.convert_ids_to_tokens(disc_id)\n",
    "\n",
    "\n",
    "                mask_idx = (outputs.disc_labels[0] == 1).nonzero(as_tuple = True)[0].tolist()\n",
    "\n",
    "                # 가짜 토큰 표시    \n",
    "                for i in mask_idx:\n",
    "                    origin_tokens_for_print[i] = \"(\" + origin_tokens_for_print[i] + \")\"\n",
    "                    disc_tokens[i] = \"(\" + disc_tokens[i] + \")\"\n",
    "\n",
    "                # 가짜 토큰 index\n",
    "                fake_idx = (outputs.disc_labels[0][:pad_idx] == 1).nonzero(as_tuple=True)[0].tolist()\n",
    "                prd_idx = (outputs.disc_predictions[0][:pad_idx] == 1).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "\n",
    "                # l2 = 가짜 토큰을 진짜 토큰으로 판단한 경우(오답)\n",
    "                l2 = []\n",
    "                for i in fake_idx :\n",
    "                    l2.append([i,origin_tokens[i],disc_tokens[i]])\n",
    "\n",
    "                # l3 = 진짜 토큰을 가짜 토큰으로 판단한 경우(오답)\n",
    "                l3 = []\n",
    "                for i in prd_idx :\n",
    "                    l3.append([i,origin_tokens[i],disc_tokens[i]])\n",
    "\n",
    "                # l1 = 가짜 토큰을 가짜토큰으로 판단한 경우(정답)\n",
    "                l1 = []\n",
    "                x = l2.copy()\n",
    "                y = l3.copy()\n",
    "                for i in x :\n",
    "                    for j in y : \n",
    "                        if i == j :\n",
    "                            l2.pop(l2.index(i))\n",
    "                            l3.pop(l3.index(j))\n",
    "                            l1.append(i)\n",
    "                            break\n",
    "\n",
    "                # l1 = 가짜 토큰을 가짜토큰으로 판단한 경우(정답)\n",
    "                l1 = list(map(lambda x : x+['fake']+['fake']+['O'],l1))\n",
    "\n",
    "                # l2 = 가짜 토큰을 진짜 토큰으로 판단한 경우(오답)\n",
    "                l2 = list(map(lambda x : x+['fake']+['-']+['X'],l2))\n",
    "\n",
    "                # l3 = 진짜 토큰을 가짜 토큰으로 판단한 경우(오답)\n",
    "                l3 = list(map(lambda x : x+['-']+['fake']+['X'],l3))\n",
    "\n",
    "                x = pd.DataFrame(l1+l2+l3)\n",
    "\n",
    "                if len(x) != 0 :\n",
    "                    x.columns = ['문장 위치','실제 토큰','(가짜)토큰','실제','예측','정답']\n",
    "\n",
    "\n",
    "                ### --- \n",
    "                print('')\n",
    "                print('')\n",
    "                print('원본 문장 : ',tokenizer.convert_tokens_to_string(origin_tokens_for_print[1:-1]))\n",
    "                print('가짜 문장 : ',tokenizer.convert_tokens_to_string(disc_tokens[1:-1]))\n",
    "                print('')\n",
    "                print('')\n",
    "                print(f'{len(origin_id)}개 토큰 중 {len(l2+l3)}개 예측 실패 -------- {len(fake_idx)}개 가짜 토큰 중 {len(l1)}개 판별')\n",
    "                display(HTML(x.to_html()))\n",
    "                print(f'Combined Loss {round(outputs.loss.item(),3)} -- Generator Loss : {round(outputs.mlm_loss.item(),3)} -- Discriminator Loss : {round(outputs.disc_loss.item(),3)}')\n",
    "                \n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = customtrainer(\n",
    "    model=model.to(device),\n",
    "    train_dataset=train_data_set,\n",
    "    eval_dataset=validation_data_set,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[myCallback],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
